{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nNVi1ldwg0P"
   },
   "source": [
    "# **FLOATING OBJECT DETECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGTfVAbC649k"
   },
   "source": [
    "**About the dataset**\n",
    "\n",
    "\n",
    "1. Dataset size?\n",
    "2. Size of images?\n",
    "3. How many categories?\n",
    "4. Exist annotation file with no data\n",
    "5. Six categories: human, wind/sup-board, boat, bouy, sailboat, kayak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_AGGoxYw-6z"
   },
   "source": [
    "**[Download dataset](https://www.kaggle.com/datasets/jangsienicajzkowy/afo-aerial-dataset-of-floating-objects/data)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DKjZI04svts3",
    "ExecuteTime": {
     "end_time": "2025-03-24T01:05:11.545944Z",
     "start_time": "2025-03-24T01:05:11.542982Z"
    }
   },
   "source": [
    "import shutil\n",
    "from pathlib import Path"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqfDovVgy_Dp"
   },
   "source": [
    "**Data path**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DB72kVt9zEvT",
    "ExecuteTime": {
     "end_time": "2025-03-24T01:05:11.548255Z",
     "start_time": "2025-03-24T01:05:11.546787Z"
    }
   },
   "source": [
    "# Image path of PART 1,2,3\n",
    "img_path_1 = 'dataset/PART_1/PART_1/images/'\n",
    "img_path_2 = 'dataset/PART_2/PART_2/images/'\n",
    "img_path_3 = 'dataset/PART_3/PART_3/images/'\n",
    "\n",
    "# Categories path\n",
    "# Categories: human, wind/sup-board, boat, bouy, sailboat, kayak\n",
    "categories_path = 'dataset/PART_1/PART_1/6categories/'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HA049S2Ar25"
   },
   "source": [
    "**Split Data into Train, Test & Validation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2dqKWAsNAq5L",
    "ExecuteTime": {
     "end_time": "2025-03-24T01:05:11.551548Z",
     "start_time": "2025-03-24T01:05:11.548775Z"
    }
   },
   "source": [
    "# Split into three parts: the training (67,4% of objects), the test (19,12% of objects),\n",
    "# and the validation set (13,48% of objects). In order to prevent overfitting of the model to the given data,\n",
    "# the test set contains selected frames from nine videos that were not used in either the training or validation sets.\n",
    "\n",
    "# Split image to : dataset/working/images\n",
    "# Split annotation to: dataset/working/labels\n",
    "\n",
    "def split_data(file_list, img_path, ann_path, mode):\n",
    "    #Check if we have our mode folders\n",
    "    images_working_folder = Path( 'dataset/working/images/'+  mode)\n",
    "    if not images_working_folder.exists():\n",
    "        print(f\"Path {images_working_folder} does not exit\")\n",
    "        os.makedirs(images_working_folder)\n",
    "\n",
    "    labels_working_folder = Path('dataset/working/labels/' + mode)\n",
    "    if not labels_working_folder.exists():\n",
    "        print(f\"Path {labels_working_folder} does not exit\")\n",
    "        os.makedirs(labels_working_folder)\n",
    "\n",
    "    #Creates the name of our label file from the img name and creates our source file\n",
    "    for file in file_list:\n",
    "        name = file.replace('.jpg', '')\n",
    "        img_src_file = str(img_path) + '/' + name + '.jpg'\n",
    "        annot_src_file = str(ann_path) + '/' + name + '.txt'\n",
    "        \n",
    "        if Path(img_src_file).exists() and Path(annot_src_file).exists():\n",
    "            #move image\n",
    "            IMG_DIR = 'dataset/working/images/' + mode\n",
    "            img_dest_file = str(IMG_DIR) + '/' + name + '.jpg'\n",
    "            if os.path.isfile(img_src_file) and not Path(img_dest_file).exists():\n",
    "                shutil.move(img_src_file, img_dest_file)\n",
    "    \n",
    "            # Copy annotations\n",
    "            ANNOT_DIR = 'dataset/working/labels/' + mode\n",
    "            annot_dest_file = str(ANNOT_DIR) + '/' + name + '.txt'\n",
    "            if os.path.isfile(annot_src_file) and not Path(annot_dest_file).exists():\n",
    "                shutil.move(annot_src_file, annot_dest_file)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-g6Po3VCLX9",
    "outputId": "670cfbf0-8366-42d1-b630-b76363a3835a",
    "ExecuteTime": {
     "end_time": "2025-03-24T01:05:11.555917Z",
     "start_time": "2025-03-24T01:05:11.552163Z"
    }
   },
   "source": [
    "#Get our images list\n",
    "train_imgs = 'dataset/PART_1/PART_1/train.txt'\n",
    "test_imgs = 'dataset/PART_1/PART_1/test.txt'\n",
    "val_imgs = 'dataset/PART_1/PART_1/validation.txt'\n",
    "with open(train_imgs, 'r') as f:\n",
    "    train_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(test_imgs, 'r') as f:\n",
    "    test_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(val_imgs, 'r') as f:\n",
    "    val_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(train_img_list[0], test_img_list[0], val_img_list[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_102.jpg k2_38.jpg a_101.jpg\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IVYECDgBOsr",
    "outputId": "e73fc9e0-c326-42e0-fd29-e61c25f3b953",
    "ExecuteTime": {
     "end_time": "2025-03-24T01:05:11.578977Z",
     "start_time": "2025-03-24T01:05:11.557176Z"
    }
   },
   "source": [
    "# Root path\n",
    "root_img_path = Path('dataset/images/')\n",
    "root_ann_path = Path('dataset/annotations/')\n",
    "\n",
    "#Split Data\n",
    "split_data(train_img_list, root_img_path, root_ann_path, 'train')\n",
    "split_data(test_img_list, root_img_path, root_ann_path, 'test')\n",
    "split_data(val_img_list, root_img_path, root_ann_path, 'val')"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:05:11.601200Z",
     "start_time": "2025-03-24T01:05:11.579511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "working_image_path = 'dataset/working/images/'\n",
    "working_labels_path = 'dataset/working/labels/'\n",
    "\n",
    "# Images\n",
    "img_test_path = glob.glob(os.path.join(working_image_path + '/test/' , \"*.jpg\"))\n",
    "print(f'img_test_path: {len(img_test_path)}')\n",
    "\n",
    "img_train_path = glob.glob(os.path.join(working_image_path + '/train/' , \"*.jpg\"))\n",
    "print(f'img_train_path: {len(img_train_path)}')\n",
    "\n",
    "img_val_path = glob.glob(os.path.join(working_image_path + '/val/' , \"*.jpg\"))\n",
    "print(f'img_val_path: {len(img_val_path)}')\n",
    "\n",
    "# Labels\n",
    "label_test_path = glob.glob(os.path.join(working_labels_path + '/test/' , \"*.txt\"))\n",
    "print(f'label_test_path: {len(label_test_path)}')\n",
    "\n",
    "label_train_path = glob.glob(os.path.join(working_labels_path + '/train/' , \"*.txt\"))\n",
    "print(f'label_train_path: {len(label_train_path)}')\n",
    "\n",
    "label_val_path = glob.glob(os.path.join(working_image_path + '/val/' , \"*.txt\"))\n",
    "print(f'label_val_path: {len(label_val_path)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_test_path: 514\n",
      "img_train_path: 2787\n",
      "img_val_path: 339\n",
      "label_test_path: 514\n",
      "label_train_path: 2787\n",
      "label_val_path: 0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Train model**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:05:12.870876Z",
     "start_time": "2025-03-24T01:05:11.602088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skimage.feature import hog\n",
    "from skimage import color\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "class ImprovedObjectDetector:\n",
    "    \"\"\"\n",
    "    Enhanced object detector for floating objects using HOG features, \n",
    "    color histograms, PCA, and SVM with multi-scale detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_names, pca_components=100, n_jobs=-1):\n",
    "        self.class_names = class_names\n",
    "        self.num_classes = len(class_names)\n",
    "        self.pca_components = pca_components\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        # Define colors for visualization\n",
    "        self.colors = [\n",
    "            (255, 0, 0),    # Red for human\n",
    "            (0, 255, 0),    # Green for wind/sup-board  \n",
    "            (0, 0, 255),    # Blue for boat\n",
    "            (255, 255, 0),  # Yellow for buoy\n",
    "            (255, 0, 255),  # Magenta for sailboat\n",
    "            (0, 255, 255)   # Cyan for kayak\n",
    "        ]\n",
    "        \n",
    "        # Create a pipeline with hyperparameter search\n",
    "        self.base_classifier = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=pca_components, random_state=42)),\n",
    "            ('svm', SVC(probability=True, random_state=42))\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, image, feature_types=None):\n",
    "        \"\"\"\n",
    "        Extract multiple types of features from an image\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        image : numpy.ndarray\n",
    "            Input image\n",
    "        feature_types : list\n",
    "            List of feature types to extract (default: ['hog', 'color'])\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Concatenated feature vector\n",
    "        \"\"\"\n",
    "        if feature_types is None:\n",
    "            feature_types = ['hog', 'color']\n",
    "            \n",
    "        # Resize to a fixed size\n",
    "        resized = cv2.resize(image, (64, 64))\n",
    "        features = []\n",
    "        \n",
    "        if 'hog' in feature_types:\n",
    "            # Extract HOG features\n",
    "            # Convert to grayscale if image is in color\n",
    "            if len(resized.shape) == 3:\n",
    "                gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                gray = resized\n",
    "                \n",
    "            hog_features = hog(gray, \n",
    "                              orientations=9,\n",
    "                              pixels_per_cell=(8, 8),\n",
    "                              cells_per_block=(2, 2),\n",
    "                              block_norm='L2-Hys',\n",
    "                              visualize=False)\n",
    "            features.append(hog_features)\n",
    "            \n",
    "        if 'color' in feature_types and len(resized.shape) == 3:\n",
    "            # Extract color histogram features from each channel\n",
    "            hsv = cv2.cvtColor(resized, cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            # Compute histograms for each channel\n",
    "            h_hist = cv2.calcHist([hsv], [0], None, [32], [0, 180])\n",
    "            s_hist = cv2.calcHist([hsv], [1], None, [32], [0, 256])\n",
    "            v_hist = cv2.calcHist([hsv], [2], None, [32], [0, 256])\n",
    "            \n",
    "            # Normalize histograms\n",
    "            h_hist = cv2.normalize(h_hist, h_hist).flatten()\n",
    "            s_hist = cv2.normalize(s_hist, s_hist).flatten()\n",
    "            v_hist = cv2.normalize(v_hist, v_hist).flatten()\n",
    "            \n",
    "            # Concatenate histograms\n",
    "            color_features = np.concatenate((h_hist, s_hist, v_hist))\n",
    "            features.append(color_features)\n",
    "        \n",
    "        # Return concatenated features\n",
    "        return np.concatenate(features) if features else np.array([])\n",
    "    \n",
    "    def load_dataset(self, image_dir, annotation_dir, subset='train'):\n",
    "        \"\"\"\n",
    "        Load dataset from images and annotations\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        image_dir : str\n",
    "            Directory containing image folders\n",
    "        annotation_dir : str\n",
    "            Directory containing annotation folders\n",
    "        subset : str\n",
    "            Dataset subset (train, test, val)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Features, labels, and bounding boxes\n",
    "        \"\"\"\n",
    "        images_path = os.path.join(image_dir, subset)\n",
    "        annotations_path = os.path.join(annotation_dir, subset)\n",
    "        \n",
    "        print(f'Loading {subset} data from:')\n",
    "        print(f'Images path: {images_path}')\n",
    "        print(f'Annotations path: {annotations_path}')\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        bboxes = []\n",
    "        img_paths = []\n",
    "        \n",
    "        print(f\"Loading {subset} dataset...\")\n",
    "        image_files = [f for f in os.listdir(images_path) if f.endswith('.jpg')]\n",
    "        \n",
    "        def process_image(img_file):\n",
    "            \"\"\"Process a single image and its annotations\"\"\"\n",
    "            local_X = []\n",
    "            local_y = []\n",
    "            local_bboxes = []\n",
    "            local_img_paths = []\n",
    "            \n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "            ann_file = os.path.join(annotations_path, base_name + '.txt')\n",
    "            \n",
    "            if not os.path.exists(ann_file):\n",
    "                print(f\"Warning: No annotation file for {img_file}\")\n",
    "                return local_X, local_y, local_bboxes, local_img_paths\n",
    "            \n",
    "            # Read image\n",
    "            img_path = os.path.join(images_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {img_path}\")\n",
    "                return local_X, local_y, local_bboxes, local_img_paths\n",
    "            \n",
    "            height, width = img.shape[:2]\n",
    "            \n",
    "            # Read annotations\n",
    "            with open(ann_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    try:\n",
    "                        class_id = int(parts[0])\n",
    "                        \n",
    "                        # YOLO format is center_x, center_y, width, height (normalized)\n",
    "                        center_x = float(parts[1])\n",
    "                        center_y = float(parts[2])\n",
    "                        bbox_width = float(parts[3])\n",
    "                        bbox_height = float(parts[4])\n",
    "                        \n",
    "                        # Convert to pixel coordinates\n",
    "                        x1 = int((center_x - bbox_width/2) * width)\n",
    "                        y1 = int((center_y - bbox_height/2) * height)\n",
    "                        x2 = int((center_x + bbox_width/2) * width)\n",
    "                        y2 = int((center_y + bbox_height/2) * height)\n",
    "                        \n",
    "                        # Ensure coordinates are within image\n",
    "                        x1 = max(0, x1)\n",
    "                        y1 = max(0, y1)\n",
    "                        x2 = min(width, x2)\n",
    "                        y2 = min(height, y2)\n",
    "                        \n",
    "                        # Only process if bbox has area\n",
    "                        if x2 > x1 and y2 > y1:\n",
    "                            # Extract object patch\n",
    "                            object_img = img[y1:y2, x1:x2]\n",
    "                            \n",
    "                            if object_img.size > 0:\n",
    "                                # Extract features\n",
    "                                features = self.extract_features(object_img)\n",
    "                                \n",
    "                                local_X.append(features)\n",
    "                                local_y.append(class_id)\n",
    "                                local_bboxes.append((x1, y1, x2-x1, y2-y1))  # x, y, width, height\n",
    "                                local_img_paths.append(img_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing annotation: {line.strip()} - {e}\")\n",
    "            \n",
    "            return local_X, local_y, local_bboxes, local_img_paths\n",
    "        \n",
    "        # Process images in parallel\n",
    "        results = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(process_image)(img_file) for img_file in tqdm(image_files)\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        for local_X, local_y, local_bboxes, local_img_paths in results:\n",
    "            X.extend(local_X)\n",
    "            y.extend(local_y)\n",
    "            bboxes.extend(local_bboxes)\n",
    "            img_paths.extend(local_img_paths)\n",
    "        \n",
    "        print(f\"Loaded {len(X)} samples from {subset} set\")\n",
    "        \n",
    "        return np.array(X), np.array(y), bboxes, img_paths\n",
    "\n",
    "    def train(self, image_dir, annotation_dir, model_save_path=\"improved_object_detector.pkl\", tune_hyperparams=True):\n",
    "        # Load training data\n",
    "        X_train, y_train, _, _ = self.load_dataset(image_dir, annotation_dir, 'train')\n",
    "        \n",
    "        if X_train.shape[0] == 0:\n",
    "            raise ValueError(\"No training samples found!\")\n",
    "        \n",
    "        print(f\"Training with {len(X_train)} samples...\")\n",
    "        \n",
    "        if tune_hyperparams:\n",
    "            print(\"Performing hyperparameter tuning...\")\n",
    "            # Define parameter grid\n",
    "            param_grid = {\n",
    "                'pca__n_components': [min(50, X_train.shape[1]), min(100, X_train.shape[1]), min(150, X_train.shape[1])],\n",
    "                'svm__C': [0.1, 1, 10],\n",
    "                'svm__gamma': ['scale', 'auto'],\n",
    "                'svm__kernel': ['rbf', 'linear']\n",
    "            }\n",
    "            \n",
    "            # Create grid search\n",
    "            self.classifier = GridSearchCV(\n",
    "                self.base_classifier,\n",
    "                param_grid,\n",
    "                cv=3,\n",
    "                n_jobs=self.n_jobs,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Train with grid search\n",
    "            self.classifier.fit(X_train, y_train)\n",
    "            print(f\"Best parameters: {self.classifier.best_params_}\")\n",
    "            print(f\"Best CV score: {self.classifier.best_score_:.4f}\")\n",
    "        else:\n",
    "            # Train with default parameters\n",
    "            self.classifier = self.base_classifier\n",
    "            self.classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Validate on validation set\n",
    "        X_val, y_val, _, _ = self.load_dataset(image_dir, annotation_dir, 'val')\n",
    "        \n",
    "        if X_val.shape[0] > 0:\n",
    "            accuracy = self.classifier.score(X_val, y_val)\n",
    "            print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "        else:\n",
    "            print(\"No validation samples found!\")\n",
    "        \n",
    "        # Save the model\n",
    "        with open(model_save_path, 'wb') as f:\n",
    "            pickle.dump(self.classifier, f)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Load a trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_path : str\n",
    "            Path to the saved model\n",
    "        \"\"\"\n",
    "        with open(model_path, 'rb') as f:\n",
    "            self.classifier = pickle.load(f)\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    def detect(self, image, confidence_threshold=0.5, nms_threshold=0.3, \n",
    "               multi_scale=True, scale_factors=None):\n",
    "        \"\"\"\n",
    "        Detect objects in an image using sliding window approach\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        image : numpy.ndarray\n",
    "            Input image\n",
    "        confidence_threshold : float\n",
    "            Minimum confidence score to keep a detection\n",
    "        nms_threshold : float\n",
    "            Non-maximum suppression threshold\n",
    "        multi_scale : bool\n",
    "            Whether to use multi-scale detection\n",
    "        scale_factors : list\n",
    "            List of scale factors for multi-scale detection\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of (class_id, confidence, bbox) tuples\n",
    "        \"\"\"\n",
    "        if scale_factors is None:\n",
    "            scale_factors = [0.5, 0.75, 1.0, 1.25, 1.5]\n",
    "        \n",
    "        height, width = image.shape[:2]\n",
    "        detections = []\n",
    "        \n",
    "        # Define window parameters\n",
    "        base_window_size = (128, 128)\n",
    "        step_size = 64\n",
    "        \n",
    "        # Process at different scales if multi_scale is True\n",
    "        scales = scale_factors if multi_scale else [1.0]\n",
    "        \n",
    "        for scale in scales:\n",
    "            # Scale the image\n",
    "            if scale != 1.0:\n",
    "                scaled_width = int(width * scale)\n",
    "                scaled_height = int(height * scale)\n",
    "                scaled_img = cv2.resize(image, (scaled_width, scaled_height))\n",
    "            else:\n",
    "                scaled_img = image\n",
    "                scaled_width = width\n",
    "                scaled_height = height\n",
    "            \n",
    "            # Adjust window and step size for this scale\n",
    "            window_width = int(base_window_size[0])\n",
    "            window_height = int(base_window_size[1])\n",
    "            scaled_step = int(step_size)\n",
    "            \n",
    "            # Sliding window detection\n",
    "            for y in range(0, scaled_height - window_height, scaled_step):\n",
    "                for x in range(0, scaled_width - window_width, scaled_step):\n",
    "                    # Extract window\n",
    "                    window = scaled_img[y:y + window_height, x:x + window_width]\n",
    "                    \n",
    "                    # Skip if window is too small\n",
    "                    if window.shape[0] < 16 or window.shape[1] < 16:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = self.extract_features(window)\n",
    "                    if features.size == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    features = features.reshape(1, -1)\n",
    "                    \n",
    "                    # Predict class and confidence\n",
    "                    probabilities = self.classifier.predict_proba(features)[0]\n",
    "                    class_id = np.argmax(probabilities)\n",
    "                    confidence = probabilities[class_id]\n",
    "                    \n",
    "                    if confidence >= confidence_threshold:\n",
    "                        # Convert back to original image coordinates\n",
    "                        orig_x = int(x / scale)\n",
    "                        orig_y = int(y / scale)\n",
    "                        orig_width = int(window_width / scale)\n",
    "                        orig_height = int(window_height / scale)\n",
    "                        \n",
    "                        detections.append((class_id, confidence, (orig_x, orig_y, orig_width, orig_height)))\n",
    "        \n",
    "        # Apply non-maximum suppression\n",
    "        detections = self._non_max_suppression(detections, nms_threshold)\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def _non_max_suppression(self, detections, threshold):\n",
    "        \"\"\"\n",
    "        Apply non-maximum suppression to remove overlapping detections\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        detections : list\n",
    "            List of (class_id, confidence, bbox) tuples\n",
    "        threshold : float\n",
    "            IoU threshold for suppression\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            Filtered list of detections\n",
    "        \"\"\"\n",
    "        if not detections:\n",
    "            return []\n",
    "        \n",
    "        # Group detections by class\n",
    "        class_detections = {}\n",
    "        for detection in detections:\n",
    "            class_id = detection[0]\n",
    "            if class_id not in class_detections:\n",
    "                class_detections[class_id] = []\n",
    "            class_detections[class_id].append(detection)\n",
    "        \n",
    "        # Apply NMS for each class separately\n",
    "        final_detections = []\n",
    "        for class_id, class_dets in class_detections.items():\n",
    "            # Sort by confidence (highest first)\n",
    "            class_dets = sorted(class_dets, key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Initialize the list of picked detections\n",
    "            picked = []\n",
    "            \n",
    "            # Extract coordinates\n",
    "            confidences = [d[1] for d in class_dets]\n",
    "            boxes = [d[2] for d in class_dets]\n",
    "            \n",
    "            # Extract coordinates\n",
    "            x1 = np.array([box[0] for box in boxes])\n",
    "            y1 = np.array([box[1] for box in boxes])\n",
    "            x2 = np.array([box[0] + box[2] for box in boxes])\n",
    "            y2 = np.array([box[1] + box[3] for box in boxes])\n",
    "            \n",
    "            # Calculate areas\n",
    "            area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "            idxs = np.argsort(confidences)[::-1]\n",
    "            \n",
    "            while len(idxs) > 0:\n",
    "                last = len(idxs) - 1\n",
    "                i = idxs[0]\n",
    "                picked.append(i)\n",
    "                \n",
    "                # Find the largest (x, y) coordinates for the start of the bounding box\n",
    "                xx1 = np.maximum(x1[i], x1[idxs[1:]])\n",
    "                yy1 = np.maximum(y1[i], y1[idxs[1:]])\n",
    "                \n",
    "                # Find the smallest (x, y) coordinates for the end of the bounding box\n",
    "                xx2 = np.minimum(x2[i], x2[idxs[1:]])\n",
    "                yy2 = np.minimum(y2[i], y2[idxs[1:]])\n",
    "                \n",
    "                # Width and height of bounding box\n",
    "                w = np.maximum(0, xx2 - xx1 + 1)\n",
    "                h = np.maximum(0, yy2 - yy1 + 1)\n",
    "                \n",
    "                # Calculate the overlap ratio (IoU)\n",
    "                overlap = (w * h) / area[idxs[1:]]\n",
    "                \n",
    "                # Delete all indexes from the index list that have overlap greater than the threshold\n",
    "                idxs = np.delete(idxs, np.concatenate(([0], np.where(overlap > threshold)[0] + 1)))\n",
    "            \n",
    "            # Add picked detections to final list\n",
    "            final_detections.extend([class_dets[i] for i in picked])\n",
    "        \n",
    "        return final_detections\n",
    "    \n",
    "    def evaluate(self, image_dir, annotation_dir, subset='test', iou_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Evaluate the detector on a dataset\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        image_dir : str\n",
    "            Directory containing image folders\n",
    "        annotation_dir : str\n",
    "            Directory containing annotation folders\n",
    "        subset : str\n",
    "            Dataset subset (train, test, val)\n",
    "        iou_threshold : float\n",
    "            IoU threshold for a true positive\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        # Load test data\n",
    "        _, _, ground_truth_boxes, img_paths = self.load_dataset(image_dir, annotation_dir, subset)\n",
    "        \n",
    "        # Group ground truth boxes by image path\n",
    "        gt_by_image = {}\n",
    "        for i, img_path in enumerate(img_paths):\n",
    "            if img_path not in gt_by_image:\n",
    "                gt_by_image[img_path] = []\n",
    "            gt_by_image[img_path].append(ground_truth_boxes[i])\n",
    "        \n",
    "        # Evaluate on each image\n",
    "        total_tp = 0\n",
    "        total_fp = 0\n",
    "        total_gt = sum(len(boxes) for boxes in gt_by_image.values())\n",
    "        \n",
    "        for img_path, gt_boxes in tqdm(gt_by_image.items(), desc=f\"Evaluating on {subset}\"):\n",
    "            # Read image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read image {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Detect objects\n",
    "            detections = self.detect(img, confidence_threshold=0.5)\n",
    "            \n",
    "            # Calculate IoU for each detection with each ground truth box\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            \n",
    "            # Create a copy of gt_boxes to mark matched boxes\n",
    "            unmatched_gt = gt_boxes.copy()\n",
    "            \n",
    "            for _, _, det_bbox in detections:\n",
    "                # Convert detection bbox to format [x1, y1, x2, y2]\n",
    "                det_x1, det_y1, det_w, det_h = det_bbox\n",
    "                det_x2, det_y2 = det_x1 + det_w, det_y1 + det_h\n",
    "                \n",
    "                best_iou = 0\n",
    "                best_idx = -1\n",
    "                \n",
    "                for i, gt_bbox in enumerate(unmatched_gt):\n",
    "                    # Convert gt bbox to format [x1, y1, x2, y2]\n",
    "                    gt_x1, gt_y1, gt_w, gt_h = gt_bbox\n",
    "                    gt_x2, gt_y2 = gt_x1 + gt_w, gt_y1 + gt_h\n",
    "                    \n",
    "                    # Calculate IoU\n",
    "                    inter_x1 = max(det_x1, gt_x1)\n",
    "                    inter_y1 = max(det_y1, gt_y1)\n",
    "                    inter_x2 = min(det_x2, gt_x2)\n",
    "                    inter_y2 = min(det_y2, gt_y2)\n",
    "                    \n",
    "                    inter_w = max(0, inter_x2 - inter_x1 + 1)\n",
    "                    inter_h = max(0, inter_y2 - inter_y1 + 1)\n",
    "                    \n",
    "                    inter_area = inter_w * inter_h\n",
    "                    \n",
    "                    det_area = det_w * det_h\n",
    "                    gt_area = gt_w * gt_h\n",
    "                    \n",
    "                    iou = inter_area / float(det_area + gt_area - inter_area)\n",
    "                    \n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_idx = i\n",
    "                \n",
    "                # Check if detection matches any ground truth box\n",
    "                if best_iou >= iou_threshold:\n",
    "                    tp += 1\n",
    "                    # Remove matched ground truth box\n",
    "                    unmatched_gt.pop(best_idx)\n",
    "                else:\n",
    "                    fp += 1\n",
    "            \n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "        \n",
    "        # Calculate precision, recall, F1\n",
    "        precision = total_tp / max(1, (total_tp + total_fp))\n",
    "        recall = total_tp / max(1, total_gt)\n",
    "        f1 = 2 * precision * recall / max(1e-8, precision + recall)\n",
    "        \n",
    "        metrics = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'true_positives': total_tp,\n",
    "            'false_positives': total_fp,\n",
    "            'total_ground_truth': total_gt\n",
    "        }\n",
    "        \n",
    "        print(f\"Evaluation results on {subset} set:\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def visualize_detections(self, image, detections, output_path=None, show_confidence=True):\n",
    "        \"\"\"\n",
    "        Visualize detections on the image\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        image : numpy.ndarray\n",
    "            Input image\n",
    "        detections : list\n",
    "            List of (class_id, confidence, bbox) tuples\n",
    "        output_path : str, optional\n",
    "            Path to save the output image\n",
    "        show_confidence : bool\n",
    "            Whether to display confidence scores\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Image with visualized detections\n",
    "        \"\"\"\n",
    "        output_img = image.copy()\n",
    "        \n",
    "        for class_id, confidence, bbox in detections:\n",
    "            x, y, w, h = bbox\n",
    "            color = self.colors[class_id % len(self.colors)]\n",
    "            class_name = self.class_names[class_id]\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(output_img, (x, y), (x + w, y + h), color, 2)\n",
    "            \n",
    "            # Prepare label text\n",
    "            if show_confidence:\n",
    "                label = f\"{class_name}: {confidence:.2f}\"\n",
    "            else:\n",
    "                label = class_name\n",
    "                \n",
    "            # Get text size\n",
    "            text_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            text_width, text_height = text_size\n",
    "            \n",
    "            # Draw label background\n",
    "            cv2.rectangle(output_img, (x, y - text_height - 5), (x + text_width, y), color, -1)\n",
    "            \n",
    "            # Draw label text\n",
    "            cv2.putText(output_img, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "        \n",
    "        if output_path:\n",
    "            cv2.imwrite(output_path, output_img)\n",
    "        \n",
    "        return output_img\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:05:12.874795Z",
     "start_time": "2025-03-24T01:05:12.871630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demo function to test the detector on a single image\n",
    "def demo_detection(image_path, model_path, class_names, output_path=None):\n",
    "    \"\"\"\n",
    "    Demonstrate object detection on a single image\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image_path : str\n",
    "        Path to the input image\n",
    "    model_path : str\n",
    "        Path to the saved model\n",
    "    class_names : list\n",
    "        List of class names\n",
    "    output_path : str, optional\n",
    "        Path to save the output image\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not read image {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Create detector\n",
    "    detector = ImprovedObjectDetector(class_names=class_names)\n",
    "    \n",
    "    # Load model\n",
    "    detector.load_model(model_path)\n",
    "    \n",
    "    # Detect objects\n",
    "    detections = detector.detect(image, \n",
    "                               confidence_threshold=0.5, \n",
    "                               nms_threshold=0.3,\n",
    "                               multi_scale=True)\n",
    "    \n",
    "    # Visualize detections\n",
    "    output_img = detector.visualize_detections(image, detections, output_path)\n",
    "    \n",
    "    print(f\"Detected {len(detections)} objects\")\n",
    "    for i, (class_id, confidence, bbox) in enumerate(detections):\n",
    "        print(f\"  {i+1}. {class_names[class_id]} with confidence {confidence:.2f} at position {bbox}\")\n",
    "    \n",
    "    # Display result if running in a GUI environment\n",
    "    try:\n",
    "        cv2.imshow(\"Detections\", output_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return output_img"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T01:23:11.119355Z",
     "start_time": "2025-03-24T01:05:12.875725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories\n",
    "    image_dir = 'dataset/working/images/'\n",
    "    annotation_dir = 'dataset/working/labels/'\n",
    "    \n",
    "    \n",
    "    class_names = [\"human\", \"wind/sup-board\", \"boat\", \"buoy\", \"sailboat\", \"kayak\"]\n",
    "    \n",
    "    # Create detector\n",
    "    detector = ImprovedObjectDetector(class_names=class_names)\n",
    "    \n",
    "    # Train model (uncomment to train)\n",
    "    detector.train(image_dir, annotation_dir, \"improved_object_detector.pkl\", tune_hyperparams=True)\n",
    "    \n",
    "    # Or load pre-trained model\n",
    "    detector.load_model(\"improved_object_detector.pkl\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    # metrics = detector.evaluate(image_dir, annotation_dir, subset='test')\n",
    "    \n",
    "    # Test on single image\n",
    "    test_image_path = 'dataset/working/images/val/a_101.jpg'\n",
    "    demo_detection(test_image_path, \"improved_object_detector.pkl\", class_names, \"output.jpg\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from:\n",
      "Images path: dataset/working/images/train\n",
      "Annotations path: dataset/working/labels/train\n",
      "Loading train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2787/2787 [00:11<00:00, 239.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26960 samples from train set\n",
      "Training with 26960 samples...\n",
      "Performing hyperparameter tuning...\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/truongngo/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Python(71337) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     11\u001B[39m detector = ImprovedObjectDetector(class_names=class_names)\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Train model (uncomment to train)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[43mdetector\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mannotation_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mimproved_object_detector.pkl\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtune_hyperparams\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# Or load pre-trained model\u001B[39;00m\n\u001B[32m     17\u001B[39m detector.load_model(\u001B[33m\"\u001B[39m\u001B[33mimproved_object_detector.pkl\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 253\u001B[39m, in \u001B[36mImprovedObjectDetector.train\u001B[39m\u001B[34m(self, image_dir, annotation_dir, model_save_path, tune_hyperparams)\u001B[39m\n\u001B[32m    244\u001B[39m \u001B[38;5;28mself\u001B[39m.classifier = GridSearchCV(\n\u001B[32m    245\u001B[39m     \u001B[38;5;28mself\u001B[39m.base_classifier,\n\u001B[32m    246\u001B[39m     param_grid,\n\u001B[32m   (...)\u001B[39m\u001B[32m    249\u001B[39m     verbose=\u001B[32m1\u001B[39m\n\u001B[32m    250\u001B[39m )\n\u001B[32m    252\u001B[39m \u001B[38;5;66;03m# Train with grid search\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m253\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.classifier.best_params_\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    255\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest CV score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.classifier.best_score_\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1024\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1018\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1019\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1020\u001B[39m     )\n\u001B[32m   1022\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1026\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1027\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1028\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1571\u001B[39m, in \u001B[36mGridSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1569\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1570\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1571\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:970\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    962\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    963\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    964\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    965\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    966\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    967\u001B[39m         )\n\u001B[32m    968\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m970\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    974\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    978\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    979\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    982\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    983\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    984\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    985\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    986\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    988\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m    989\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    990\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    993\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     72\u001B[39m config = get_config()\n\u001B[32m     73\u001B[39m iterable_with_config = (\n\u001B[32m     74\u001B[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     76\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/joblib/parallel.py:2007\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2001\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2002\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2003\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2004\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2005\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2007\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/joblib/parallel.py:1650\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1647\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1649\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1650\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1652\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1653\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1654\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1655\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1656\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/joblib/parallel.py:1762\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._jobs) == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[32m   1760\u001B[39m     (\u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(\n\u001B[32m   1761\u001B[39m         timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING)):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1763\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1765\u001B[39m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[32m   1766\u001B[39m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[32m   1767\u001B[39m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
