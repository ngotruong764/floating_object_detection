{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nNVi1ldwg0P"
   },
   "source": [
    "# **FLOATING OBJECT DETECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGTfVAbC649k"
   },
   "source": [
    "**About the dataset**\n",
    "\n",
    "\n",
    "1. Dataset size?\n",
    "2. Size of images?\n",
    "3. How many categories?\n",
    "4. Exist annotation file with no data\n",
    "5. Six categories: human, wind/sup-board, boat, bouy, sailboat, kayak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_AGGoxYw-6z"
   },
   "source": [
    "**[Download dataset](https://www.kaggle.com/datasets/jangsienicajzkowy/afo-aerial-dataset-of-floating-objects/data)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DKjZI04svts3",
    "ExecuteTime": {
     "end_time": "2025-03-24T16:13:30.235314Z",
     "start_time": "2025-03-24T16:13:30.231944Z"
    }
   },
   "source": [
    "import shutil\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqfDovVgy_Dp"
   },
   "source": [
    "**Data path**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DB72kVt9zEvT",
    "ExecuteTime": {
     "end_time": "2025-03-24T16:13:30.252618Z",
     "start_time": "2025-03-24T16:13:30.250707Z"
    }
   },
   "source": [
    "# Image path of PART 1,2,3\n",
    "img_path_1 = 'dataset/PART_1/PART_1/images/'\n",
    "img_path_2 = 'dataset/PART_2/PART_2/images/'\n",
    "img_path_3 = 'dataset/PART_3/PART_3/images/'\n",
    "\n",
    "# Categories path\n",
    "# Categories: human, wind/sup-board, boat, bouy, sailboat, kayak\n",
    "categories_path = 'dataset/PART_1/PART_1/6categories/'"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HA049S2Ar25"
   },
   "source": [
    "**Split Data into Train, Test & Validation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2dqKWAsNAq5L",
    "ExecuteTime": {
     "end_time": "2025-03-24T16:13:30.281641Z",
     "start_time": "2025-03-24T16:13:30.278595Z"
    }
   },
   "source": [
    "# Split into three parts: the training (67,4% of objects), the test (19,12% of objects),\n",
    "# and the validation set (13,48% of objects). In order to prevent overfitting of the model to the given data,\n",
    "# the test set contains selected frames from nine videos that were not used in either the training or validation sets.\n",
    "\n",
    "# Split image to : dataset/working/images\n",
    "# Split annotation to: dataset/working/labels\n",
    "\n",
    "def split_data(file_list, img_path, ann_path, mode):\n",
    "    #Check if we have our mode folders\n",
    "    images_working_folder = Path( 'dataset/working/images/'+  mode)\n",
    "    if not images_working_folder.exists():\n",
    "        print(f\"Path {images_working_folder} does not exit\")\n",
    "        os.makedirs(images_working_folder)\n",
    "\n",
    "    labels_working_folder = Path('dataset/working/labels/' + mode)\n",
    "    if not labels_working_folder.exists():\n",
    "        print(f\"Path {labels_working_folder} does not exit\")\n",
    "        os.makedirs(labels_working_folder)\n",
    "\n",
    "    #Creates the name of our label file from the img name and creates our source file\n",
    "    for file in file_list:\n",
    "        name = file.replace('.jpg', '')\n",
    "        img_src_file = str(img_path) + '/' + name + '.jpg'\n",
    "        annot_src_file = str(ann_path) + '/' + name + '.txt'\n",
    "        \n",
    "        if Path(img_src_file).exists() and Path(annot_src_file).exists():\n",
    "            #move image\n",
    "            IMG_DIR = 'dataset/working/images/' + mode\n",
    "            img_dest_file = str(IMG_DIR) + '/' + name + '.jpg'\n",
    "            if os.path.isfile(img_src_file) and not Path(img_dest_file).exists():\n",
    "                shutil.move(img_src_file, img_dest_file)\n",
    "    \n",
    "            # Copy annotations\n",
    "            ANNOT_DIR = 'dataset/working/labels/' + mode\n",
    "            annot_dest_file = str(ANNOT_DIR) + '/' + name + '.txt'\n",
    "            if os.path.isfile(annot_src_file) and not Path(annot_dest_file).exists():\n",
    "                shutil.move(annot_src_file, annot_dest_file)"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-g6Po3VCLX9",
    "outputId": "670cfbf0-8366-42d1-b630-b76363a3835a",
    "ExecuteTime": {
     "end_time": "2025-03-24T16:13:30.286867Z",
     "start_time": "2025-03-24T16:13:30.282516Z"
    }
   },
   "source": [
    "#Get our images list\n",
    "train_imgs = 'dataset/PART_1/PART_1/train.txt'\n",
    "test_imgs = 'dataset/PART_1/PART_1/test.txt'\n",
    "val_imgs = 'dataset/PART_1/PART_1/validation.txt'\n",
    "with open(train_imgs, 'r') as f:\n",
    "    train_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(test_imgs, 'r') as f:\n",
    "    test_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(val_imgs, 'r') as f:\n",
    "    val_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(train_img_list[0], test_img_list[0], val_img_list[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_102.jpg k2_38.jpg a_101.jpg\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IVYECDgBOsr",
    "outputId": "e73fc9e0-c326-42e0-fd29-e61c25f3b953",
    "ExecuteTime": {
     "end_time": "2025-03-24T16:13:30.324882Z",
     "start_time": "2025-03-24T16:13:30.295417Z"
    }
   },
   "source": [
    "# Root path\n",
    "root_img_path = Path('dataset/images/')\n",
    "root_ann_path = Path('dataset/annotations/')\n",
    "\n",
    "#Split Data\n",
    "split_data(train_img_list, root_img_path, root_ann_path, 'train')\n",
    "split_data(test_img_list, root_img_path, root_ann_path, 'test')\n",
    "split_data(val_img_list, root_img_path, root_ann_path, 'val')"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:13:30.347592Z",
     "start_time": "2025-03-24T16:13:30.325915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "working_image_path = 'dataset/working/images/'\n",
    "working_labels_path = 'dataset/working/labels/'\n",
    "\n",
    "# Images\n",
    "img_test_path = glob.glob(os.path.join(working_image_path + '/test/' , \"*.jpg\"))\n",
    "print(f'img_test_path: {len(img_test_path)}')\n",
    "\n",
    "img_train_path = glob.glob(os.path.join(working_image_path + '/train/' , \"*.jpg\"))\n",
    "print(f'img_train_path: {len(img_train_path)}')\n",
    "\n",
    "img_val_path = glob.glob(os.path.join(working_image_path + '/val/' , \"*.jpg\"))\n",
    "print(f'img_val_path: {len(img_val_path)}')\n",
    "\n",
    "# Labels\n",
    "label_test_path = glob.glob(os.path.join(working_labels_path + '/test/' , \"*.txt\"))\n",
    "print(f'label_test_path: {len(label_test_path)}')\n",
    "\n",
    "label_train_path = glob.glob(os.path.join(working_labels_path + '/train/' , \"*.txt\"))\n",
    "print(f'label_train_path: {len(label_train_path)}')\n",
    "\n",
    "label_val_path = glob.glob(os.path.join(working_image_path + '/val/' , \"*.txt\"))\n",
    "print(f'label_val_path: {len(label_val_path)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_test_path: 514\n",
      "img_train_path: 2787\n",
      "img_val_path: 339\n",
      "label_test_path: 514\n",
      "label_train_path: 2787\n",
      "label_val_path: 0\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Train model**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:13:30.369096Z",
     "start_time": "2025-03-24T16:13:30.348828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "class CustomObjectDetector:\n",
    "    def __init__(self, n_components=50, n_clusters=20, confidence_threshold=0.8, debug_mode=False):\n",
    "    \n",
    "        self.n_components = n_components\n",
    "        self.n_clusters = n_clusters\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.debug_mode = debug_mode\n",
    "        \n",
    "        # Initialize the models\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters)\n",
    "        self.classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        \n",
    "        # Class names for visualization\n",
    "        self.class_names = ['human', 'wind/sup-board', 'boat', 'buoy', 'sailboat', 'kayak']\n",
    "        \n",
    "        # Colors for visualization (one for each class)\n",
    "        self.colors = [\n",
    "            (0, 255, 0),    # Green for human\n",
    "            (255, 0, 0),    # Blue for wind/sup-board\n",
    "            (0, 0, 255),    # Red for boat\n",
    "            (255, 255, 0),  # Cyan for buoy\n",
    "            (255, 0, 255),  # Magenta for sailboat\n",
    "            (0, 255, 255)   # Yellow for kayak\n",
    "        ]\n",
    "    \n",
    "    def train(self, image_folder, annotation_folder):\n",
    "        print(\"Loading training data...\")\n",
    "        X_train, y_train, _, _ = self.load_dataset(image_folder, annotation_folder, 'train', debug_visualization=False)\n",
    "        \n",
    "        if len(X_train) == 0:\n",
    "            raise ValueError(\"No training data found. Check your paths and data format.\")\n",
    "\n",
    "        print(f\"Loaded {len(X_train)} training samples\")\n",
    "\n",
    "        # Apply PCA for dimensionality reduction\n",
    "        print(\"Applying PCA...\")\n",
    "        X_train_pca = self.pca.fit_transform(X_train)\n",
    "\n",
    "        # Train k-means for feature clustering\n",
    "        print(\"Training K-means clustering...\")\n",
    "        self.kmeans.fit(X_train_pca)\n",
    "\n",
    "        # Add cluster information to features\n",
    "        cluster_features = self.kmeans.transform(X_train_pca)\n",
    "        X_train_with_clusters = np.hstack([X_train_pca, cluster_features])\n",
    "\n",
    "        # Train the classifier\n",
    "        print(\"Training the classifier...\")\n",
    "        self.classifier.fit(X_train_with_clusters, y_train)\n",
    "\n",
    "        # Evaluate on validation set if available\n",
    "        try:\n",
    "            print(\"Evaluating on test data...\")\n",
    "            # X_val, y_val, _, _ = self.load_dataset(image_folder, annotation_folder, 'val')\n",
    "            X_test, y_test, _, _ = self.load_dataset(image_folder, annotation_folder, 'test')\n",
    "            if len(X_test) > 0:\n",
    "                X_test_pca = self.pca.transform(X_test)\n",
    "                cluster_test = self.kmeans.transform(X_test_pca)\n",
    "                X_test_with_clusters = np.hstack([X_test_pca, cluster_test])\n",
    "                y_pred = self.classifier.predict(X_test_with_clusters)\n",
    "                print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "                print(\"\\nClassification Report:\")\n",
    "                print(classification_report(y_test, y_pred, target_names=self.class_names))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not evaluate on validation data: {e}\")\n",
    "        \n",
    "    \n",
    "    def load_dataset(self, image_folder, annotation_folder, split_type='train', debug_visualization=False):\n",
    "        print(\"Loading dataset...\")\n",
    "        X = []\n",
    "        y = []\n",
    "        bbox_data = []\n",
    "        image_paths = []\n",
    "        \n",
    "        img_dir = os.path.join(image_folder, split_type)\n",
    "        ann_dir = os.path.join(annotation_folder, split_type)\n",
    "        \n",
    "        image_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        for img_file in tqdm(image_files):\n",
    "            # Get the corresponding annotation file\n",
    "            ann_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "            ann_path = os.path.join(ann_dir, ann_file)\n",
    "            \n",
    "            # Skip if annotation file doesn't exist\n",
    "            if not os.path.exists(ann_path):\n",
    "                continue\n",
    "            \n",
    "            # Load image\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            image = cv2.imread(img_path)\n",
    "            \n",
    "            if image is None:\n",
    "                print(f\"Warning: Could not read image {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            image_height, image_width = image.shape[:2]\n",
    "            image_paths.append(img_path)\n",
    "            # print(f'Image path: {img_path}')\n",
    "            \n",
    "            # Read annotations\n",
    "            with open(ann_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Process each object in the image\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                    \n",
    "                class_id = int(parts[0])\n",
    "                # YOLO format: center_x, center_y, width, height (normalized)\n",
    "                x_center = float(parts[1]) * image_width\n",
    "                y_center = float(parts[2]) * image_height\n",
    "                width = float(parts[3]) * image_width\n",
    "                height = float(parts[4]) * image_height\n",
    "\n",
    "                # Convert to top-left, bottom-right coordinates\n",
    "                x1 = max(0, int(x_center - width / 2))\n",
    "                y1 = max(0, int(y_center - height / 2))\n",
    "                x2 = min(image_width, int(x_center + width / 2))\n",
    "                y2 = min(image_height, int(y_center + height / 2))\n",
    "\n",
    "                # Extract the object region (ROI)\n",
    "                object_img = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "                \n",
    "                # Extract features\n",
    "                features = self.extract_features(object_img)\n",
    "                X.append(features)\n",
    "                y.append(class_id)\n",
    "                bbox_data.append((img_path, class_id, int(x1), int(y1), int(x2), int(y2)))\n",
    "                \n",
    "                # Extract the object region with error handling\n",
    "                try:\n",
    "                    # Visual debugging if requested\n",
    "                    if debug_visualization:\n",
    "                        plt.figure(figsize=(10, 5))\n",
    "                        plt.subplot(1, 2, 1)\n",
    "                        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "                        plt.plot([x1, x2, x2, x1, x1], [y1, y1, y2, y2, y1], 'r-')\n",
    "                        plt.title(f\"Image with box: {self.class_names[class_id]}\")\n",
    "                        \n",
    "                        plt.subplot(1, 2, 2)\n",
    "                        plt.imshow(cv2.cvtColor(object_img, cv2.COLOR_BGR2RGB))\n",
    "                        plt.title(f\"Extracted region: {self.class_names[class_id]}\")\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing region in {img_path}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "        return np.array(X), np.array(y), bbox_data, image_paths\n",
    "    \n",
    "    def extract_features(self, image):\n",
    "        # Convert to grayscale\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "            \n",
    "        # Resize to a standard size\n",
    "        resized = cv2.resize(gray, (100, 100))\n",
    "        \n",
    "        # Apply some basic features (without using pre-built model)\n",
    "        # 1. Histogram of oriented gradients (simplified)\n",
    "        gx = cv2.Sobel(resized, cv2.CV_32F, 1, 0)\n",
    "        gy = cv2.Sobel(resized, cv2.CV_32F, 0, 1)\n",
    "        mag, ang = cv2.cartToPolar(gx, gy)\n",
    "        \n",
    "        # 2. Intensity histogram\n",
    "        hist = cv2.calcHist([resized], [0], None, [32], [0, 256])\n",
    "        \n",
    "        # 3. Local binary patterns (simplified)\n",
    "        lbp = np.zeros_like(resized)\n",
    "        for i in range(1, resized.shape[0] - 1):\n",
    "            for j in range(1, resized.shape[1] - 1):\n",
    "                center = resized[i, j]\n",
    "                code = 0\n",
    "                code |= (resized[i-1, j-1] >= center) << 0\n",
    "                code |= (resized[i-1, j] >= center) << 1\n",
    "                code |= (resized[i-1, j+1] >= center) << 2\n",
    "                code |= (resized[i, j+1] >= center) << 3\n",
    "                code |= (resized[i+1, j+1] >= center) << 4\n",
    "                code |= (resized[i+1, j] >= center) << 5\n",
    "                code |= (resized[i+1, j-1] >= center) << 6\n",
    "                code |= (resized[i, j-1] >= center) << 7\n",
    "                lbp[i, j] = code\n",
    "        \n",
    "        lbp_hist = cv2.calcHist([lbp.astype(np.uint8)], [0], None, [32], [0, 256])\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = np.concatenate([\n",
    "            mag.flatten(),\n",
    "            ang.flatten(),\n",
    "            hist.flatten(),\n",
    "            lbp_hist.flatten()\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        model_data = {\n",
    "            'pca': self.pca,\n",
    "            'kmeans': self.kmeans,\n",
    "            'classifier': self.classifier,\n",
    "            'n_components': self.n_components,\n",
    "            'n_clusters': self.n_clusters,\n",
    "            'class_names': self.class_names,\n",
    "            'colors': self.colors,\n",
    "            'confidence_threshold': self.confidence_threshold\n",
    "        }\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "    def find_regions_of_interest(self, image):\n",
    "        \n",
    "         # Convert to HSV for color-based detection\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # Step 1: Create color masks\n",
    "        # White/light colored\n",
    "        lower_white = np.array([0, 0, 180])\n",
    "        upper_white = np.array([180, 30, 255])\n",
    "        white_mask = cv2.inRange(hsv, lower_white, upper_white)\n",
    "        \n",
    "        lower_blue = np.array([90, 50, 50])\n",
    "        upper_blue = np.array([130, 255, 255])\n",
    "        blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "        \n",
    "        # Combined mask\n",
    "        boat_mask = cv2.bitwise_or(white_mask, blue_mask)\n",
    "        \n",
    "        # Clean up mask\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        boat_mask = cv2.morphologyEx(boat_mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "        boat_mask = cv2.morphologyEx(boat_mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "        \n",
    "        # Step 2: Find contours in the mask\n",
    "        contours, _ = cv2.findContours(boat_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Step 3: Filter contours based on size, shape, and area\n",
    "        detected_objects = []\n",
    "        \n",
    "        for contour in contours:\n",
    "            # Get bounding rectangle\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            \n",
    "            area = cv2.contourArea(contour)\n",
    "                \n",
    "            # Check if shape is appropriate for a boat\n",
    "            if area > 200:\n",
    "                # Add padding\n",
    "                padding = 10\n",
    "                x = max(0, x - padding)\n",
    "                y = max(0, y - padding)\n",
    "                w = min(image.shape[1] - x, w + 2*padding)\n",
    "                h = min(image.shape[0] - y, h + 2*padding)\n",
    "                \n",
    "                confidence = min(area / 1000, 0.99)\n",
    "                detected_objects.append((x, y, w, h, confidence))\n",
    "                \n",
    "        \n",
    "        # Step 4: If insufficient detections from color, try structural analysis\n",
    "        if len(detected_objects) < 1:\n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # Enhance contrast\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "            enhanced = clahe.apply(gray)\n",
    "            \n",
    "            # Use bilateral filter to preserve edges but smooth areas\n",
    "            filtered = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
    "            \n",
    "            # Use Canny edge detector to find edges\n",
    "            edges = cv2.Canny(filtered, 30, 150)\n",
    "            \n",
    "            # Dilate to connect edges\n",
    "            dilated = cv2.dilate(edges, kernel, iterations=1)\n",
    "            \n",
    "            # Find contours in the edges\n",
    "            contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Filter contours\n",
    "            for contour in contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour) \n",
    "                aspect_ratio = float(w) / h\n",
    "                area = cv2.contourArea(contour)\n",
    "                \n",
    "                # Boats typically have an aspect ratio between 1:1 and 3:1\n",
    "                if 1.0 <= aspect_ratio <= 3.0 and area > 300:\n",
    "                    # Add padding\n",
    "                    padding = 10\n",
    "                    x = max(0, x - padding)\n",
    "                    y = max(0, y - padding)\n",
    "                    w = min(image.shape[1] - x, w + 2*padding)\n",
    "                    h = min(image.shape[0] - y, h + 2*padding)\n",
    "                    \n",
    "                    confidence = min(area / 2000, 0.7)  # Lower confidence for this method\n",
    "                    detected_objects.append((x, y, w, h, confidence))\n",
    "        \n",
    "        # Debug visualization if needed\n",
    "        # if self.debug_mode:\n",
    "        #     debug_img = image.copy()\n",
    "        #     for x, y, w, h, conf in detected_objects:\n",
    "        #         cv2.rectangle(debug_img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        #         cv2.putText(debug_img, f\"Object {conf:.2f}\", (x, y-5), \n",
    "        #                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        #     \n",
    "        #     plt.figure(figsize=(10, 8))\n",
    "        #     plt.subplot(121)\n",
    "        #     plt.imshow(boat_mask, cmap='gray')\n",
    "        #     plt.title(\"Object Detection Mask\")\n",
    "        #     plt.subplot(122)\n",
    "        #     plt.imshow(debug_img)\n",
    "        #     plt.title(\"Object Detections\")\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "        \n",
    "        return detected_objects\n",
    "            \n",
    "    def detect_objects(self, image_rgb, image):\n",
    "        # Find regions of interest\n",
    "        regions = self.find_regions_of_interest(image_rgb)\n",
    "        detections = []\n",
    "        \n",
    "        # Process each region\n",
    "        for i, region in enumerate(regions):\n",
    "            x, y, w, h, conf = region\n",
    "            \n",
    "            # Extract region\n",
    "            try:\n",
    "                region_img = image[y:y+h, x:x+w]\n",
    "                # Skip if region is too small\n",
    "                if region_img.shape[0] < 10 or region_img.shape[1] < 10:\n",
    "                    continue\n",
    "                \n",
    "                # Extract features\n",
    "                features = self.extract_features(region_img)\n",
    "                features = features.reshape(1, -1)\n",
    "                \n",
    "                # Apply PCA\n",
    "                features_pca = self.pca.transform(features)\n",
    "                \n",
    "                # Get cluster features\n",
    "                cluster_features = self.kmeans.transform(features_pca)\n",
    "                features_with_clusters = np.hstack([features_pca, cluster_features])\n",
    "                \n",
    "                # Predict class and confidence\n",
    "                class_id = self.classifier.predict(features_with_clusters)[0]\n",
    "                confidence = np.max(self.classifier.predict_proba(features_with_clusters)[0])\n",
    "                \n",
    "                if confidence > self.confidence_threshold:\n",
    "                    detections.append((class_id, confidence, x, y, w, h))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing region {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Step 5: Perform non-maximum suppression\n",
    "        object_detections = self._non_max_suppression(detections)\n",
    "        # Filter by confidence threshold\n",
    "        object_detections = [obj for obj in object_detections if obj[2] >= self.confidence_threshold]\n",
    "        \n",
    "        return object_detections\n",
    "    \n",
    "    \n",
    "    def process_image(self, image_path, output_dir=\"output\"):\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Get base filename without extension\n",
    "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not read image at {image_path}\")\n",
    "            return\n",
    "        \n",
    "        # Convert to RGB\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Find regions of interest\n",
    "        # object_detection = self.find_regions_of_interest(image_rgb)\n",
    "        \n",
    "        # Detect objects\n",
    "        detections = self.detect_objects(image_rgb, image)\n",
    "        print(f\"Found {len(detections)} objects\")\n",
    "        \n",
    "        # Visualize detections\n",
    "        result_image = self.visualize_detection(image, detections)\n",
    "        detection_output_path = os.path.join(output_dir, f\"{base_name}_detection.jpg\")\n",
    "        cv2.imwrite(detection_output_path, result_image)\n",
    "        print(f\"Detection result saved to {detection_output_path}\")\n",
    "        \n",
    "        # Draw detections on the visualization image\n",
    "        # self._draw_detections(visualization, detections)\n",
    "        # \n",
    "        # # Save or display results\n",
    "        # if output_dir:\n",
    "        #     cv2.imwrite(output_dir, cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR))\n",
    "        #     print(f\"Results saved to {output_dir}\")\n",
    "        #     \n",
    "        # elif self.debug_mode:\n",
    "        #     plt.figure(figsize=(15, 10))\n",
    "        #     plt.imshow(visualization)\n",
    "        #     plt.title(\"Object Detections\")\n",
    "        #     plt.axis('off')\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def _calculate_iou(self, box1, box2):\n",
    "        \"\"\"\n",
    "        Calculate Intersection over Union for two boxes.\n",
    "        \n",
    "        Args:\n",
    "            box1: Tuple of (x1, y1, x2, y2)\n",
    "            box2: Tuple of (x1, y1, x2, y2)\n",
    "            \n",
    "        Returns:\n",
    "            float: IoU value\n",
    "        \"\"\"\n",
    "        # Calculate intersection area\n",
    "        x_left = max(box1[0], box2[0])\n",
    "        y_top = max(box1[1], box2[1])\n",
    "        x_right = min(box1[2], box2[2])\n",
    "        y_bottom = min(box1[3], box2[3])\n",
    "        \n",
    "        if x_right < x_left or y_bottom < y_top:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "        \n",
    "        # Calculate area of both boxes\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        \n",
    "        # Calculate IoU\n",
    "        iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "        return iou\n",
    "    \n",
    "    def _non_max_suppression(self, boxes, overlap_threshold=0.3):\n",
    "        if not boxes:\n",
    "            return []\n",
    "        \n",
    "        # Convert (x, y, w, h) to (x1, y1, x2, y2) format\n",
    "        boxes_xyxy = [(class_id, conf ,x, y, x+w, y+h) for class_id, conf, x, y, w, h in boxes]\n",
    "        \n",
    "        # Sort by confidence (highest first)\n",
    "        boxes_xyxy.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        keep = []\n",
    "        \n",
    "        while boxes_xyxy:\n",
    "            # Take the box with highest confidence\n",
    "            current = boxes_xyxy.pop(0)\n",
    "            keep.append(current)\n",
    "            \n",
    "            # Check remaining boxes\n",
    "            i = 0\n",
    "            while i < len(boxes_xyxy):\n",
    "                # Calculate IoU between current box and this box\n",
    "                iou = self._calculate_iou(current[2:6], boxes_xyxy[i][2:6])\n",
    "\n",
    "                \n",
    "                if iou > overlap_threshold:\n",
    "                    # Remove box\n",
    "                    boxes_xyxy.pop(i)\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        # Convert back to (x, y, w, h, confidence) format\n",
    "        result = [(class_id, conf, x1, y1, x2-x1, y2-y1) for class_id, conf, x1, y1, x2, y2, in keep]\n",
    "        return result\n",
    "    \n",
    "    def visualize_detection(self, image, detections):\n",
    "        result_image = image.copy()\n",
    "        \n",
    "        # Sort detections by confidence (highest first)\n",
    "        sorted_detections = sorted(detections, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for detection in sorted_detections:\n",
    "            class_id, confidence, x, y, w, h = detection\n",
    "            x1, y1, x2, y2 = int(x), int(y), int(x + w), int(y + h)\n",
    "            class_id = int(class_id)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            color = self.colors[class_id]\n",
    "            cv2.rectangle(result_image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "            \n",
    "            # Create label with class name and confidence\n",
    "            label = f\"{self.class_names[class_id]}: {confidence:.2f}\"\n",
    "            \n",
    "            # Get text size for better label placement\n",
    "            (label_width, label_height), baseline = cv2.getTextSize(\n",
    "                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2\n",
    "            )\n",
    "            \n",
    "            # Draw label background\n",
    "            cv2.rectangle(\n",
    "                result_image, \n",
    "                (int(x1), int(y1) - label_height - 10), \n",
    "                (int(x1) + label_width, int(y1)), \n",
    "                color, \n",
    "                -1  # Filled rectangle\n",
    "            )\n",
    "            \n",
    "            # Draw label text in white\n",
    "            cv2.putText(\n",
    "                result_image, \n",
    "                label, \n",
    "                (int(x1), int(y1) - 7),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, \n",
    "                (255, 255, 255), \n",
    "                2\n",
    "            )\n",
    "        \n",
    "        # Add metadata\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        info_text = f\"Detections: {len(detections)} | Time: {timestamp} | Threshold: {self.confidence_threshold}\"\n",
    "        cv2.putText(\n",
    "            result_image,\n",
    "            info_text,\n",
    "            (10, result_image.shape[0] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (0, 0, 0),\n",
    "            2\n",
    "        )\n",
    "        \n",
    "        return result_image\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:13:30.372179Z",
     "start_time": "2025-03-24T16:13:30.370107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_path):\n",
    "        \"\"\"\n",
    "        Load a trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the model\n",
    "            \n",
    "        Returns:\n",
    "            detector: Loaded detector\n",
    "        \"\"\"\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        detector = CustomObjectDetector(\n",
    "            n_components=model_data['n_components'],\n",
    "            n_clusters=model_data['n_clusters'],\n",
    "            confidence_threshold=model_data['confidence_threshold']\n",
    "        )\n",
    "        \n",
    "        detector.pca = model_data['pca']\n",
    "        detector.kmeans = model_data['kmeans']\n",
    "        detector.classifier = model_data['classifier']\n",
    "        detector.class_names = model_data['class_names']\n",
    "        detector.colors = model_data['colors']\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        return detector"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:47:08.415068Z",
     "start_time": "2025-03-24T16:13:30.373093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define paths\n",
    "image_folder = 'dataset/working/images'  # Base folder containing train, test, val\n",
    "annotation_folder = 'dataset/working/labels'  # Base folder containing train, test, val\n",
    "model_path = 'custom_detector1.pkl'\n",
    "\n",
    "# Create and train the detector\n",
    "detector = CustomObjectDetector()\n",
    "detector.train(image_folder, annotation_folder)\n",
    "\n",
    "# Option 2: Advanced training with background class (uncomment to use)\n",
    "# detector.train_with_background(image_folder, annotation_folder)\n",
    "\n",
    "# Save the model\n",
    "detector.save_model(model_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2787/2787 [25:40<00:00,  1.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26960 training samples\n",
      "Applying PCA...\n",
      "Training K-means clustering...\n",
      "Training the classifier...\n",
      "Evaluating on test data...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [07:28<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8858489331064275\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         human       0.94      0.97      0.95      6511\n",
      "wind/sup-board       0.53      0.43      0.48       634\n",
      "          boat       0.02      0.01      0.01       143\n",
      "          buoy       0.90      0.15      0.26        59\n",
      "      sailboat       0.00      0.00      0.00        28\n",
      "         kayak       0.49      0.53      0.51       264\n",
      "\n",
      "      accuracy                           0.89      7639\n",
      "     macro avg       0.48      0.35      0.37      7639\n",
      "  weighted avg       0.87      0.89      0.87      7639\n",
      "\n",
      "Model saved to custom_detector1.pkl\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:47:11.573262Z",
     "start_time": "2025-03-24T16:47:08.416516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loaded_detector = load_model('custom_detector1.pkl')\n",
    "image_path = \"dataset/working/images/val/a_1013.jpg\"\n",
    "loaded_detector.process_image(image_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from custom_detector1.pkl\n",
      "Found 36 objects\n",
      "Detection result saved to output/a_1013_detection.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.int64(0), np.float64(1.0), 562, 1564, 39, 44),\n",
       " (np.int64(0), np.float64(1.0), 2870, 1452, 50, 52),\n",
       " (np.int64(0), np.float64(1.0), 820, 1324, 46, 56),\n",
       " (np.int64(0), np.float64(1.0), 732, 1312, 70, 51),\n",
       " (np.int64(0), np.float64(1.0), 747, 1277, 60, 43),\n",
       " (np.int64(0), np.float64(1.0), 1215, 477, 59, 39),\n",
       " (np.int64(0), np.float64(1.0), 2743, 409, 50, 39),\n",
       " (np.int64(0), np.float64(1.0), 2129, 374, 37, 67),\n",
       " (np.int64(0), np.float64(0.99), 2189, 1534, 56, 47),\n",
       " (np.int64(0), np.float64(0.99), 646, 1321, 55, 55),\n",
       " (np.int64(0), np.float64(0.99), 2759, 826, 72, 77),\n",
       " (np.int64(0), np.float64(0.99), 1630, 548, 45, 36),\n",
       " (np.int64(0), np.float64(0.99), 2830, 370, 54, 59),\n",
       " (np.int64(0), np.float64(0.98), 1058, 1225, 55, 58),\n",
       " (np.int64(1), np.float64(0.98), 1014, 698, 267, 182),\n",
       " (np.int64(0), np.float64(0.98), 2750, 431, 58, 46),\n",
       " (np.int64(0), np.float64(0.97), 726, 1242, 48, 38),\n",
       " (np.int64(0), np.float64(0.97), 1536, 225, 58, 49),\n",
       " (np.int64(0), np.float64(0.96), 205, 1778, 33, 43),\n",
       " (np.int64(0), np.float64(0.96), 138, 1351, 48, 35),\n",
       " (np.int64(0), np.float64(0.95), 2934, 1493, 57, 71),\n",
       " (np.int64(0), np.float64(0.95), 1639, 1437, 63, 64),\n",
       " (np.int64(0), np.float64(0.93), 160, 1232, 42, 36),\n",
       " (np.int64(0), np.float64(0.93), 566, 1161, 58, 49),\n",
       " (np.int64(0), np.float64(0.93), 2998, 978, 43, 40),\n",
       " (np.int64(0), np.float64(0.93), 1545, 601, 58, 59),\n",
       " (np.int64(1), np.float64(0.92), 1467, 363, 269, 134),\n",
       " (np.int64(0), np.float64(0.9), 1214, 1521, 50, 51),\n",
       " (np.int64(0), np.float64(0.89), 120, 1778, 45, 36),\n",
       " (np.int64(0), np.float64(0.88), 104, 1300, 65, 62),\n",
       " (np.int64(0), np.float64(0.88), 131, 695, 129, 90),\n",
       " (np.int64(0), np.float64(0.86), 1679, 1584, 42, 48),\n",
       " (np.int64(0), np.float64(0.86), 1166, 1536, 43, 56),\n",
       " (np.int64(0), np.float64(0.85), 250, 1748, 59, 47),\n",
       " (np.int64(0), np.float64(0.85), 1517, 1337, 66, 81),\n",
       " (np.int64(0), np.float64(0.82), 2531, 487, 94, 85)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
