{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nNVi1ldwg0P"
   },
   "source": [
    "# **FLOATING OBJECT DETECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGTfVAbC649k"
   },
   "source": [
    "**About the dataset**\n",
    "\n",
    "\n",
    "1. Dataset size?\n",
    "2. Size of images?\n",
    "3. How many categories?\n",
    "4. Exist annotation file with no data\n",
    "5. Six categories: human, wind/sup-board, boat, bouy, sailboat, kayak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_AGGoxYw-6z"
   },
   "source": [
    "**[Download dataset](https://www.kaggle.com/datasets/jangsienicajzkowy/afo-aerial-dataset-of-floating-objects/data)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DKjZI04svts3",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:35:23.908160Z",
     "start_time": "2025-03-24T10:35:23.863049Z"
    }
   },
   "source": [
    "import shutil\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqfDovVgy_Dp"
   },
   "source": [
    "**Data path**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DB72kVt9zEvT",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:35:23.926996Z",
     "start_time": "2025-03-24T10:35:23.913916Z"
    }
   },
   "source": [
    "# Image path of PART 1,2,3\n",
    "img_path_1 = 'dataset/PART_1/PART_1/images/'\n",
    "img_path_2 = 'dataset/PART_2/PART_2/images/'\n",
    "img_path_3 = 'dataset/PART_3/PART_3/images/'\n",
    "\n",
    "# Categories path\n",
    "# Categories: human, wind/sup-board, boat, bouy, sailboat, kayak\n",
    "categories_path = 'dataset/PART_1/PART_1/6categories/'"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HA049S2Ar25"
   },
   "source": [
    "**Split Data into Train, Test & Validation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2dqKWAsNAq5L",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:35:23.959446Z",
     "start_time": "2025-03-24T10:35:23.930817Z"
    }
   },
   "source": [
    "# Split into three parts: the training (67,4% of objects), the test (19,12% of objects),\n",
    "# and the validation set (13,48% of objects). In order to prevent overfitting of the model to the given data,\n",
    "# the test set contains selected frames from nine videos that were not used in either the training or validation sets.\n",
    "\n",
    "# Split image to : dataset/working/images\n",
    "# Split annotation to: dataset/working/labels\n",
    "\n",
    "def split_data(file_list, img_path, ann_path, mode):\n",
    "    #Check if we have our mode folders\n",
    "    images_working_folder = Path( 'dataset/working/images/'+  mode)\n",
    "    if not images_working_folder.exists():\n",
    "        print(f\"Path {images_working_folder} does not exit\")\n",
    "        os.makedirs(images_working_folder)\n",
    "\n",
    "    labels_working_folder = Path('dataset/working/labels/' + mode)\n",
    "    if not labels_working_folder.exists():\n",
    "        print(f\"Path {labels_working_folder} does not exit\")\n",
    "        os.makedirs(labels_working_folder)\n",
    "\n",
    "    #Creates the name of our label file from the img name and creates our source file\n",
    "    for file in file_list:\n",
    "        name = file.replace('.jpg', '')\n",
    "        img_src_file = str(img_path) + '/' + name + '.jpg'\n",
    "        annot_src_file = str(ann_path) + '/' + name + '.txt'\n",
    "        \n",
    "        if Path(img_src_file).exists() and Path(annot_src_file).exists():\n",
    "            #move image\n",
    "            IMG_DIR = 'dataset/working/images/' + mode\n",
    "            img_dest_file = str(IMG_DIR) + '/' + name + '.jpg'\n",
    "            if os.path.isfile(img_src_file) and not Path(img_dest_file).exists():\n",
    "                shutil.move(img_src_file, img_dest_file)\n",
    "    \n",
    "            # Copy annotations\n",
    "            ANNOT_DIR = 'dataset/working/labels/' + mode\n",
    "            annot_dest_file = str(ANNOT_DIR) + '/' + name + '.txt'\n",
    "            if os.path.isfile(annot_src_file) and not Path(annot_dest_file).exists():\n",
    "                shutil.move(annot_src_file, annot_dest_file)"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-g6Po3VCLX9",
    "outputId": "670cfbf0-8366-42d1-b630-b76363a3835a",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:35:24.004987Z",
     "start_time": "2025-03-24T10:35:23.964641Z"
    }
   },
   "source": [
    "#Get our images list\n",
    "train_imgs = 'dataset/PART_1/PART_1/train.txt'\n",
    "test_imgs = 'dataset/PART_1/PART_1/test.txt'\n",
    "val_imgs = 'dataset/PART_1/PART_1/validation.txt'\n",
    "with open(train_imgs, 'r') as f:\n",
    "    train_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(test_imgs, 'r') as f:\n",
    "    test_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(val_imgs, 'r') as f:\n",
    "    val_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(train_img_list[0], test_img_list[0], val_img_list[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_102.jpg k2_38.jpg a_101.jpg\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IVYECDgBOsr",
    "outputId": "e73fc9e0-c326-42e0-fd29-e61c25f3b953",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:35:24.519021Z",
     "start_time": "2025-03-24T10:35:24.007430Z"
    }
   },
   "source": [
    "# Root path\n",
    "root_img_path = Path('dataset/images/')\n",
    "root_ann_path = Path('dataset/annotations/')\n",
    "\n",
    "#Split Data\n",
    "split_data(train_img_list, root_img_path, root_ann_path, 'train')\n",
    "split_data(test_img_list, root_img_path, root_ann_path, 'test')\n",
    "split_data(val_img_list, root_img_path, root_ann_path, 'val')"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T10:35:24.690681Z",
     "start_time": "2025-03-24T10:35:24.525227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "working_image_path = 'dataset/working/images/'\n",
    "working_labels_path = 'dataset/working/labels/'\n",
    "\n",
    "# Images\n",
    "img_test_path = glob.glob(os.path.join(working_image_path + '/test/' , \"*.jpg\"))\n",
    "print(f'img_test_path: {len(img_test_path)}')\n",
    "\n",
    "img_train_path = glob.glob(os.path.join(working_image_path + '/train/' , \"*.jpg\"))\n",
    "print(f'img_train_path: {len(img_train_path)}')\n",
    "\n",
    "img_val_path = glob.glob(os.path.join(working_image_path + '/val/' , \"*.jpg\"))\n",
    "print(f'img_val_path: {len(img_val_path)}')\n",
    "\n",
    "# Labels\n",
    "label_test_path = glob.glob(os.path.join(working_labels_path + '/test/' , \"*.txt\"))\n",
    "print(f'label_test_path: {len(label_test_path)}')\n",
    "\n",
    "label_train_path = glob.glob(os.path.join(working_labels_path + '/train/' , \"*.txt\"))\n",
    "print(f'label_train_path: {len(label_train_path)}')\n",
    "\n",
    "label_val_path = glob.glob(os.path.join(working_image_path + '/val/' , \"*.txt\"))\n",
    "print(f'label_val_path: {len(label_val_path)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_test_path: 514\n",
      "img_train_path: 2787\n",
      "img_val_path: 339\n",
      "label_test_path: 514\n",
      "label_train_path: 2787\n",
      "label_val_path: 0\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Train model**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T10:35:24.804829Z",
     "start_time": "2025-03-24T10:35:24.697731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from skimage.util import img_as_float\n",
    "from skimage.segmentation import felzenszwalb, slic\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from skimage.morphology import closing, square\n",
    "\n",
    "class CustomObjectDetector:\n",
    "    def __init__(self, n_components=50, n_clusters=20, sliding_window_sizes=[(100, 100), (150, 150), (200, 200)], \n",
    "                 window_step=50, confidence_threshold=0.8):\n",
    "        \"\"\"\n",
    "        Initialize the custom object detector.\n",
    "        \n",
    "        Args:\n",
    "            n_components: Number of PCA components\n",
    "            n_clusters: Number of clusters for feature detection\n",
    "            sliding_window_sizes: List of window sizes for sliding window detection\n",
    "            window_step: Step size for sliding window\n",
    "            confidence_threshold: Threshold for detection confidence\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_clusters = n_clusters\n",
    "        self.sliding_window_sizes = sliding_window_sizes\n",
    "        self.window_step = window_step\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Initialize the models\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters)\n",
    "        self.classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        \n",
    "        # Class names for visualization\n",
    "        self.class_names = ['human', 'wind/sup-board', 'boat', 'buoy', 'sailboat', 'kayak']\n",
    "        \n",
    "        # Colors for visualization (one for each class)\n",
    "        self.colors = [\n",
    "            (0, 255, 0),    # Green for human\n",
    "            (255, 0, 0),    # Blue for wind/sup-board\n",
    "            (0, 0, 255),    # Red for boat\n",
    "            (255, 255, 0),  # Cyan for buoy\n",
    "            (255, 0, 255),  # Magenta for sailboat\n",
    "            (0, 255, 255)   # Yellow for kayak\n",
    "        ]\n",
    "    \n",
    "    def train(self, image_folder, annotation_folder):\n",
    "        \"\"\"\n",
    "        Train the object detector.\n",
    "        \n",
    "        Args:\n",
    "            image_folder: Path to the image folder\n",
    "            annotation_folder: Path to the annotation folder\n",
    "        \"\"\"\n",
    "        print(\"Loading training data...\")\n",
    "        X_train, y_train, _, _ = self.load_dataset(image_folder, annotation_folder, 'train', debug_visualization=False)\n",
    "        \n",
    "        if len(X_train) == 0:\n",
    "            raise ValueError(\"No training data found. Check your paths and data format.\")\n",
    "\n",
    "        print(f\"Loaded {len(X_train)} training samples\")\n",
    "\n",
    "        # Apply PCA for dimensionality reduction\n",
    "        print(\"Applying PCA...\")\n",
    "        X_train_pca = self.pca.fit_transform(X_train)\n",
    "\n",
    "        # Train k-means for feature clustering\n",
    "        print(\"Training K-means clustering...\")\n",
    "        self.kmeans.fit(X_train_pca)\n",
    "\n",
    "        # Add cluster information to features\n",
    "        cluster_features = self.kmeans.transform(X_train_pca)\n",
    "        X_train_with_clusters = np.hstack([X_train_pca, cluster_features])\n",
    "\n",
    "        # Train the classifier\n",
    "        print(\"Training the classifier...\")\n",
    "        self.classifier.fit(X_train_with_clusters, y_train)\n",
    "\n",
    "        # Evaluate on validation set if available\n",
    "        try:\n",
    "            print(\"Evaluating on test data...\")\n",
    "            # X_val, y_val, _, _ = self.load_dataset(image_folder, annotation_folder, 'val')\n",
    "            X_test, y_test, _, _ = self.load_dataset(image_folder, annotation_folder, 'test')\n",
    "            if len(X_test) > 0:\n",
    "                X_test_pca = self.pca.transform(X_test)\n",
    "                cluster_test = self.kmeans.transform(X_test_pca)\n",
    "                X_test_with_clusters = np.hstack([X_test_pca, cluster_test])\n",
    "                y_pred = self.classifier.predict(X_test_with_clusters)\n",
    "                print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "                print(\"\\nClassification Report:\")\n",
    "                print(classification_report(y_test, y_pred, target_names=self.class_names))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not evaluate on validation data: {e}\")\n",
    "    \n",
    "    \n",
    "    def train_with_background(self, image_folder, annotation_folder):\n",
    "        \"\"\"\n",
    "        Train classifier with an explicit background class to improve discrimination.\n",
    "        This is an advanced enhancement that requires creating negative samples.\n",
    "        \n",
    "        Args:\n",
    "            image_folder: Path to the image folder\n",
    "            annotation_folder: Path to the annotation folder\n",
    "        \"\"\"\n",
    "        print(\"Loading training data...\")\n",
    "        X_train, y_train, _, _ = self.load_dataset(image_folder, annotation_folder, 'train')\n",
    "        \n",
    "        # Get hard negative examples (background images that might confuse the classifier)\n",
    "        print(\"Mining hard negative examples...\")\n",
    "        X_hard_neg, y_hard_neg = self.mine_hard_negatives(image_folder, annotation_folder)\n",
    "        \n",
    "        # Combine regular training data with hard negatives\n",
    "        X_combined = np.vstack([X_train, X_hard_neg]) if len(X_hard_neg) > 0 else X_train\n",
    "        y_combined = np.concatenate([y_train, y_hard_neg]) if len(y_hard_neg) > 0 else y_train\n",
    "        \n",
    "        print(f\"Training with {len(X_train)} regular examples and {len(X_hard_neg) if len(X_hard_neg) > 0 else 0} hard negative examples\")\n",
    "        \n",
    "        # Apply PCA for dimensionality reduction\n",
    "        print(\"Applying PCA...\")\n",
    "        X_train_pca = self.pca.fit_transform(X_combined)\n",
    "        \n",
    "        # Train k-means for feature clustering\n",
    "        print(\"Training K-means clustering...\")\n",
    "        self.kmeans.fit(X_train_pca)\n",
    "        \n",
    "        # Add cluster information to features\n",
    "        cluster_features = self.kmeans.transform(X_train_pca)\n",
    "        X_train_with_clusters = np.hstack([X_train_pca, cluster_features])\n",
    "        \n",
    "        # Train the classifier\n",
    "        print(\"Training the classifier with background class...\")\n",
    "        self.classifier.fit(X_train_with_clusters, y_combined)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def load_dataset(self, image_folder, annotation_folder, split_type='train', debug_visualization=False):\n",
    "        \"\"\"\n",
    "        Load the dataset.\n",
    "        \n",
    "        Args:\n",
    "            image_folder: Path to the image folder\n",
    "            annotation_folder: Path to the annotation folder\n",
    "            split_type: 'train', 'test', or 'val'\n",
    "            \n",
    "        Returns:\n",
    "            X: Features\n",
    "            y: Labels\n",
    "            bbox_data: Bounding box information\n",
    "            image_paths: Paths to the images\n",
    "        \"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        X = []\n",
    "        y = []\n",
    "        bbox_data = []\n",
    "        image_paths = []\n",
    "        \n",
    "        img_dir = os.path.join(image_folder, split_type)\n",
    "        ann_dir = os.path.join(annotation_folder, split_type)\n",
    "        \n",
    "        image_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        for img_file in tqdm(image_files):\n",
    "            # Get the corresponding annotation file\n",
    "            ann_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "            ann_path = os.path.join(ann_dir, ann_file)\n",
    "            \n",
    "            # Skip if annotation file doesn't exist\n",
    "            if not os.path.exists(ann_path):\n",
    "                continue\n",
    "            \n",
    "            # Load image\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            image = cv2.imread(img_path)\n",
    "            \n",
    "            if image is None:\n",
    "                print(f\"Warning: Could not read image {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            image_height, image_width = image.shape[:2]\n",
    "            image_paths.append(img_path)\n",
    "            # print(f'Image path: {img_path}')\n",
    "            \n",
    "            # Read annotations\n",
    "            with open(ann_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Process each object in the image\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                    \n",
    "                class_id = int(parts[0])\n",
    "                # YOLO format: center_x, center_y, width, height (normalized)\n",
    "                x_center = float(parts[1]) * image_width\n",
    "                y_center = float(parts[2]) * image_height\n",
    "                width = float(parts[3]) * image_width\n",
    "                height = float(parts[4]) * image_height\n",
    "\n",
    "                # Convert to top-left, bottom-right coordinates\n",
    "                x1 = max(0, int(x_center - width / 2))\n",
    "                y1 = max(0, int(y_center - height / 2))\n",
    "                x2 = min(image_width, int(x_center + width / 2))\n",
    "                y2 = min(image_height, int(y_center + height / 2))\n",
    "                # x1 = float(image_width) * (2.0 * float(parts[1]) - float(parts[3])) / 2.0\n",
    "                # y1 = float(image_height) * (2.0 * float(parts[2]) - float(parts[4])) / 2.0\n",
    "                # x2 = float(image_width) * (2.0 * float(parts[1]) + float(parts[3])) / 2.0\n",
    "                # y2 = float(image_height) * (2.0 * float(parts[2]) + float(parts[4])) / 2.0\n",
    "\n",
    "                # Extract the object region (ROI)\n",
    "                object_img = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "                \n",
    "                # Extract features\n",
    "                features = self.extract_features(object_img)\n",
    "                X.append(features)\n",
    "                y.append(class_id)\n",
    "                bbox_data.append((img_path, class_id, int(x1), int(y1), int(x2), int(y2)))\n",
    "                \n",
    "                # Extract the object region with error handling\n",
    "                try:\n",
    "                    # Visual debugging if requested\n",
    "                    if debug_visualization:\n",
    "                        plt.figure(figsize=(10, 5))\n",
    "                        plt.subplot(1, 2, 1)\n",
    "                        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "                        plt.plot([x1, x2, x2, x1, x1], [y1, y1, y2, y2, y1], 'r-')\n",
    "                        plt.title(f\"Image with box: {self.class_names[class_id]}\")\n",
    "                        \n",
    "                        plt.subplot(1, 2, 2)\n",
    "                        plt.imshow(cv2.cvtColor(object_img, cv2.COLOR_BGR2RGB))\n",
    "                        plt.title(f\"Extracted region: {self.class_names[class_id]}\")\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing region in {img_path}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "        return np.array(X), np.array(y), bbox_data, image_paths\n",
    "    \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract features from an image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            \n",
    "        Returns:\n",
    "            features: Extracted features\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        if len(image.shape) == 3:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = image\n",
    "            \n",
    "        # Resize to a standard size\n",
    "        resized = cv2.resize(gray, (100, 100))\n",
    "        \n",
    "        # Apply some basic features (without using pre-built model)\n",
    "        # 1. Histogram of oriented gradients (simplified)\n",
    "        gx = cv2.Sobel(resized, cv2.CV_32F, 1, 0)\n",
    "        gy = cv2.Sobel(resized, cv2.CV_32F, 0, 1)\n",
    "        mag, ang = cv2.cartToPolar(gx, gy)\n",
    "        \n",
    "        # 2. Intensity histogram\n",
    "        hist = cv2.calcHist([resized], [0], None, [32], [0, 256])\n",
    "        \n",
    "        # 3. Local binary patterns (simplified)\n",
    "        lbp = np.zeros_like(resized)\n",
    "        for i in range(1, resized.shape[0] - 1):\n",
    "            for j in range(1, resized.shape[1] - 1):\n",
    "                center = resized[i, j]\n",
    "                code = 0\n",
    "                code |= (resized[i-1, j-1] >= center) << 0\n",
    "                code |= (resized[i-1, j] >= center) << 1\n",
    "                code |= (resized[i-1, j+1] >= center) << 2\n",
    "                code |= (resized[i, j+1] >= center) << 3\n",
    "                code |= (resized[i+1, j+1] >= center) << 4\n",
    "                code |= (resized[i+1, j] >= center) << 5\n",
    "                code |= (resized[i+1, j-1] >= center) << 6\n",
    "                code |= (resized[i, j-1] >= center) << 7\n",
    "                lbp[i, j] = code\n",
    "        \n",
    "        lbp_hist = cv2.calcHist([lbp.astype(np.uint8)], [0], None, [32], [0, 256])\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = np.concatenate([\n",
    "            mag.flatten(),\n",
    "            ang.flatten(),\n",
    "            hist.flatten(),\n",
    "            lbp_hist.flatten()\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Save the trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to save the model\n",
    "        \"\"\"\n",
    "        model_data = {\n",
    "            'pca': self.pca,\n",
    "            'kmeans': self.kmeans,\n",
    "            'classifier': self.classifier,\n",
    "            'n_components': self.n_components,\n",
    "            'n_clusters': self.n_clusters,\n",
    "            'class_names': self.class_names,\n",
    "            'colors': self.colors,\n",
    "            'confidence_threshold': self.confidence_threshold\n",
    "        }\n",
    "        \n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {model_path}\")\n",
    "            \n",
    "    def detect_objects(self, image, roi_method='felzenszwalb', expand_factor=0.1):\n",
    "        \"\"\"\n",
    "        Detect objects in an image using region-based approach.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            roi_method: Method for finding regions of interest\n",
    "            expand_factor: Factor to expand regions by\n",
    "            \n",
    "        Returns:\n",
    "            detections: List of (class_id, confidence, x1, y1, x2, y2)\n",
    "        \"\"\"\n",
    "        # Find regions of interest\n",
    "        print(f\"Finding regions of interest using {roi_method}...\")\n",
    "        regions = self.find_regions_of_interest(image, method=roi_method)\n",
    "        print(f\"Found {len(regions)} potential regions\")\n",
    "        \n",
    "        detections = []\n",
    "        image_height, image_width = image.shape[:2]\n",
    "        \n",
    "        # Process each region\n",
    "        for i, region in enumerate(regions):\n",
    "            x1, y1, x2, y2 = region\n",
    "            \n",
    "            # Expand region slightly\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            \n",
    "            # Add padding\n",
    "            x1_expanded = max(0, int(x1 - width * expand_factor))\n",
    "            y1_expanded = max(0, int(y1 - height * expand_factor))\n",
    "            x2_expanded = min(image_width, int(x2 + width * expand_factor))\n",
    "            y2_expanded = min(image_height, int(y2 + height * expand_factor))\n",
    "            \n",
    "            # Extract region\n",
    "            try:\n",
    "                region_img = image[y1_expanded:y2_expanded, x1_expanded:x2_expanded]\n",
    "                \n",
    "                # Skip if region is too small\n",
    "                if region_img.shape[0] < 10 or region_img.shape[1] < 10:\n",
    "                    continue\n",
    "                \n",
    "                # Extract features\n",
    "                features = self.extract_features(region_img)\n",
    "                features = features.reshape(1, -1)\n",
    "                \n",
    "                # Apply PCA\n",
    "                features_pca = self.pca.transform(features)\n",
    "                \n",
    "                # Get cluster features\n",
    "                cluster_features = self.kmeans.transform(features_pca)\n",
    "                features_with_clusters = np.hstack([features_pca, cluster_features])\n",
    "                \n",
    "                # Predict class and confidence\n",
    "                class_id = self.classifier.predict(features_with_clusters)[0]\n",
    "                confidence = np.max(self.classifier.predict_proba(features_with_clusters)[0])\n",
    "                \n",
    "                if confidence > self.confidence_threshold:\n",
    "                    detections.append((class_id, confidence, x1_expanded, y1_expanded, x2_expanded, y2_expanded))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing region {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Apply non-maximum suppression\n",
    "        return self.non_max_suppression(detections)\n",
    "    \n",
    "    def non_max_suppression(self, boxes, overlap_thresh=0.5):\n",
    "        \"\"\"\n",
    "        Apply non-maximum suppression to remove overlapping detections.\n",
    "        \n",
    "        Args:\n",
    "            boxes: List of (class_id, confidence, x1, y1, x2, y2)\n",
    "            overlap_thresh: Threshold for overlap\n",
    "            \n",
    "        Returns:\n",
    "            result: List of non-overlapping boxes\n",
    "        \"\"\"\n",
    "        # If no boxes, return empty list\n",
    "        if len(boxes) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        boxes = np.array(boxes)\n",
    "        \n",
    "        # Initialize the list of picked indexes\n",
    "        pick = []\n",
    "        \n",
    "        # Extract coordinates\n",
    "        class_ids = boxes[:, 0]\n",
    "        confidences = boxes[:, 1]\n",
    "        x1 = boxes[:, 2]\n",
    "        y1 = boxes[:, 3]\n",
    "        x2 = boxes[:, 4]\n",
    "        y2 = boxes[:, 5]\n",
    "        \n",
    "        # Compute area of the boxes\n",
    "        area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "        \n",
    "        # Sort by confidence (highest first)\n",
    "        idxs = np.argsort(confidences)[::-1]\n",
    "        \n",
    "        # Keep looping while some indexes still remain in the indexes list\n",
    "        while len(idxs) > 0:\n",
    "            # Grab the last index and add the index value to the list of picked indexes\n",
    "            last = len(idxs) - 1\n",
    "            i = idxs[0]\n",
    "            pick.append(i)\n",
    "            \n",
    "            # Find the largest (x, y) coordinates for the start of the bounding box\n",
    "            # and the smallest (x, y) coordinates for the end of the bounding box\n",
    "            xx1 = np.maximum(x1[i], x1[idxs[1:]])\n",
    "            yy1 = np.maximum(y1[i], y1[idxs[1:]])\n",
    "            xx2 = np.minimum(x2[i], x2[idxs[1:]])\n",
    "            yy2 = np.minimum(y2[i], y2[idxs[1:]])\n",
    "            \n",
    "            # Compute the width and height of the bounding box\n",
    "            w = np.maximum(0, xx2 - xx1 + 1)\n",
    "            h = np.maximum(0, yy2 - yy1 + 1)\n",
    "            \n",
    "            # Compute the ratio of overlap\n",
    "            overlap = (w * h) / area[idxs[1:]]\n",
    "            \n",
    "            # Delete all indexes from the index list that have overlap greater than the threshold\n",
    "            idxs = np.delete(idxs, np.concatenate(([0], np.where(overlap > overlap_thresh)[0] + 1)))\n",
    "        \n",
    "        # Return only the bounding boxes that were picked\n",
    "        result = [boxes[i] for i in pick]\n",
    "        return result\n",
    "    \n",
    "    def visualize_detection(self, image, detections):\n",
    "        \"\"\"\n",
    "        Visualize detection results.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            detections: List of (class_id, confidence, x1, y1, x2, y2)\n",
    "            \n",
    "        Returns:\n",
    "            result_image: Image with detections\n",
    "        \"\"\"\n",
    "        result_image = image.copy()\n",
    "        \n",
    "        # Sort detections by confidence (highest first)\n",
    "        sorted_detections = sorted(detections, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for detection in sorted_detections:\n",
    "            class_id, confidence, x1, y1, x2, y2 = detection\n",
    "            class_id = int(class_id)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            color = self.colors[class_id]\n",
    "            cv2.rectangle(result_image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "            \n",
    "            # Create label with class name and confidence\n",
    "            label = f\"{self.class_names[class_id]}: {confidence:.2f}\"\n",
    "            \n",
    "            # Get text size for better label placement\n",
    "            (label_width, label_height), baseline = cv2.getTextSize(\n",
    "                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2\n",
    "            )\n",
    "            \n",
    "            # Draw label background\n",
    "            cv2.rectangle(\n",
    "                result_image, \n",
    "                (int(x1), int(y1) - label_height - 10), \n",
    "                (int(x1) + label_width, int(y1)), \n",
    "                color, \n",
    "                -1  # Filled rectangle\n",
    "            )\n",
    "            \n",
    "            # Draw label text in white\n",
    "            cv2.putText(\n",
    "                result_image, \n",
    "                label, \n",
    "                (int(x1), int(y1) - 7),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, \n",
    "                (255, 255, 255), \n",
    "                2\n",
    "            )\n",
    "        \n",
    "        # Add metadata\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        info_text = f\"Detections: {len(detections)} | Time: {timestamp} | Threshold: {self.confidence_threshold}\"\n",
    "        cv2.putText(\n",
    "            result_image,\n",
    "            info_text,\n",
    "            (10, result_image.shape[0] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (0, 0, 0),\n",
    "            2\n",
    "        )\n",
    "        \n",
    "        return result_image\n",
    "    \n",
    "    \n",
    "    def visualize_regions(self, image, regions):\n",
    "        \"\"\"\n",
    "        Visualize regions of interest.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            regions: List of (x1, y1, x2, y2)\n",
    "            \n",
    "        Returns:\n",
    "            result_image: Image with regions\n",
    "        \"\"\"\n",
    "        result_image = image.copy()\n",
    "        \n",
    "        for i, region in enumerate(regions):\n",
    "            x1, y1, x2, y2 = region\n",
    "            \n",
    "            # Draw bounding box in yellow\n",
    "            cv2.rectangle(result_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 255), 1)\n",
    "            \n",
    "            # Draw region number\n",
    "            cv2.putText(\n",
    "                result_image, \n",
    "                f\"{i}\", \n",
    "                (int(x1), int(y1) - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, \n",
    "                (0, 255, 255), \n",
    "                1\n",
    "            )\n",
    "        \n",
    "        # Add info\n",
    "        cv2.putText(\n",
    "            result_image,\n",
    "            f\"Regions: {len(regions)}\",\n",
    "            (10, result_image.shape[0] - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.6,\n",
    "            (0, 0, 0),\n",
    "            2\n",
    "        )\n",
    "        \n",
    "        return result_image\n",
    "    \n",
    "    \n",
    "    def process_image(self, image_path, output_dir=\"output\", visualize_roi=True):\n",
    "        \"\"\"\n",
    "        Process an image and save detection results.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "            output_dir: Directory to save results\n",
    "            visualize_roi: Whether to visualize regions of interest\n",
    "        \"\"\"\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Get base filename without extension\n",
    "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not read image at {image_path}\")\n",
    "            return\n",
    "        \n",
    "        # Find regions of interest\n",
    "        regions = self.find_regions_of_interest(image)\n",
    "        \n",
    "        # Visualize regions if requested\n",
    "        if visualize_roi:\n",
    "            roi_image = self.visualize_regions(image, regions)\n",
    "            roi_output_path = os.path.join(output_dir, f\"{base_name}_regions.jpg\")\n",
    "            cv2.imwrite(roi_output_path, roi_image)\n",
    "            print(f\"Regions of interest saved to {roi_output_path}\")\n",
    "        \n",
    "        # Detect objects\n",
    "        detections = self.detect_objects(image)\n",
    "        print(f\"Found {len(detections)} objects\")\n",
    "        \n",
    "        # Visualize detections\n",
    "        result_image = self.visualize_detection(image, detections)\n",
    "        detection_output_path = os.path.join(output_dir, f\"{base_name}_detection.jpg\")\n",
    "        cv2.imwrite(detection_output_path, result_image)\n",
    "        print(f\"Detection result saved to {detection_output_path}\")\n",
    "        \n",
    "    \n",
    "    def find_regions_of_interest(self, image, method='felzenszwalb', min_size=100):\n",
    "        \"\"\"\n",
    "        Find regions of interest in an image using segmentation.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            method: Segmentation method ('felzenszwalb', 'slic', or 'contour')\n",
    "            min_size: Minimum size of regions to consider\n",
    "            \n",
    "        Returns:\n",
    "            regions: List of (x1, y1, x2, y2) for potential object regions\n",
    "        \"\"\"\n",
    "        # Convert to float and handle grayscale if needed\n",
    "        img_float = img_as_float(image)\n",
    "        \n",
    "        if method == 'felzenszwalb':\n",
    "            # Felzenszwalb segmentation\n",
    "            segments = felzenszwalb(img_float, scale=100, sigma=0.5, min_size=min_size)\n",
    "        elif method == 'slic':\n",
    "            # SLIC superpixel segmentation\n",
    "            segments = slic(img_float, n_segments=100, compactness=10, sigma=1, start_label=1)\n",
    "        elif method == 'contour':\n",
    "            # Edge-based contour detection\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "            edges = cv2.Canny(blurred, 50, 150)\n",
    "            \n",
    "            # Clean up edges with morphological operations\n",
    "            closed = closing(edges, square(3))\n",
    "            \n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(closed.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Create regions from contours\n",
    "            regions = []\n",
    "            for contour in contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                if w * h >= min_size:\n",
    "                    regions.append((x, y, x + w, y + h))\n",
    "            return regions\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown segmentation method: {method}\")\n",
    "        \n",
    "        # Extract regions from segments\n",
    "        regions = []\n",
    "        for segment_id in range(np.max(segments) + 1):\n",
    "            # Create a mask for this segment\n",
    "            mask = (segments == segment_id).astype(np.uint8)\n",
    "            \n",
    "            # Find contours in the mask\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # If no contours, skip\n",
    "            if not contours:\n",
    "                continue\n",
    "                \n",
    "            # Get bounding box\n",
    "            x, y, w, h = cv2.boundingRect(contours[0])\n",
    "            \n",
    "            # Filter by size\n",
    "            if w * h >= min_size:\n",
    "                regions.append((x, y, x + w, y + h))\n",
    "        \n",
    "        return regions\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T10:35:24.811883Z",
     "start_time": "2025-03-24T10:35:24.806477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_path):\n",
    "        \"\"\"\n",
    "        Load a trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the model\n",
    "            \n",
    "        Returns:\n",
    "            detector: Loaded detector\n",
    "        \"\"\"\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        detector = CustomObjectDetector(\n",
    "            n_components=model_data['n_components'],\n",
    "            n_clusters=model_data['n_clusters'],\n",
    "            confidence_threshold=model_data['confidence_threshold']\n",
    "        )\n",
    "        \n",
    "        detector.pca = model_data['pca']\n",
    "        detector.kmeans = model_data['kmeans']\n",
    "        detector.classifier = model_data['classifier']\n",
    "        detector.class_names = model_data['class_names']\n",
    "        detector.colors = model_data['colors']\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        return detector"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:26:16.146325Z",
     "start_time": "2025-03-24T10:35:24.813225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define paths\n",
    "image_folder = 'dataset/working/images'  # Base folder containing train, test, val\n",
    "annotation_folder = 'dataset/working/labels'  # Base folder containing train, test, val\n",
    "model_path = 'custom_detector1.pkl'\n",
    "\n",
    "# Create and train the detector\n",
    "detector = CustomObjectDetector()\n",
    "detector.train(image_folder, annotation_folder)\n",
    "\n",
    "# Option 2: Advanced training with background class (uncomment to use)\n",
    "# detector.train_with_background(image_folder, annotation_folder)\n",
    "\n",
    "# Save the model\n",
    "detector.save_model(model_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2787/2787 [42:23<00:00,  1.10it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26960 training samples\n",
      "Applying PCA...\n",
      "Training K-means clustering...\n",
      "Training the classifier...\n",
      "Evaluating on test data...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 514/514 [07:43<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8850634899856002\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         human       0.93      0.97      0.95      6511\n",
      "wind/sup-board       0.52      0.43      0.47       634\n",
      "          boat       0.02      0.01      0.01       143\n",
      "          buoy       1.00      0.15      0.26        59\n",
      "      sailboat       0.00      0.00      0.00        28\n",
      "         kayak       0.54      0.55      0.54       264\n",
      "\n",
      "      accuracy                           0.89      7639\n",
      "     macro avg       0.50      0.35      0.37      7639\n",
      "  weighted avg       0.87      0.89      0.87      7639\n",
      "\n",
      "Model saved to custom_detector1.pkl\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:34:15.613687Z",
     "start_time": "2025-03-24T11:26:16.155461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loaded_detector = load_model('custom_detector1.pkl')\n",
    "image_path = \"dataset/working/images/val/a_101.jpg\"  # Update with correct image path\n",
    "loaded_detector.process_image(image_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from custom_detector1.pkl\n",
      "Regions of interest saved to output/a_101_regions.jpg\n",
      "Finding regions of interest using felzenszwalb...\n",
      "Found 6340 potential regions\n",
      "Found 566 objects\n",
      "Detection result saved to output/a_101_detection.jpg\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
