{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nNVi1ldwg0P"
   },
   "source": [
    "# **FLOATING OBJECT DETECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGTfVAbC649k"
   },
   "source": [
    "**About the dataset**\n",
    "\n",
    "\n",
    "1. Dataset size?\n",
    "2. Size of images?\n",
    "3. How many categories?\n",
    "4. Exist annotation file with no data\n",
    "5. Six categories: human, wind/sup-board, boat, bouy, sailboat, kayak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_AGGoxYw-6z"
   },
   "source": [
    "**[Download dataset](https://www.kaggle.com/datasets/jangsienicajzkowy/afo-aerial-dataset-of-floating-objects/data)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DKjZI04svts3",
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.621972Z",
     "start_time": "2025-03-23T09:23:20.606176Z"
    }
   },
   "source": [
    "import shutil\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 300
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqfDovVgy_Dp"
   },
   "source": [
    "**Data path**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DB72kVt9zEvT",
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.683145Z",
     "start_time": "2025-03-23T09:23:20.679677Z"
    }
   },
   "source": [
    "# Image path of PART 1,2,3\n",
    "img_path_1 = 'dataset/PART_1/PART_1/images/'\n",
    "img_path_2 = 'dataset/PART_2/PART_2/images/'\n",
    "img_path_3 = 'dataset/PART_3/PART_3/images/'\n",
    "\n",
    "# Categories path\n",
    "# Categories: human, wind/sup-board, boat, bouy, sailboat, kayak\n",
    "categories_path = 'dataset/PART_1/PART_1/6categories/'"
   ],
   "execution_count": 301,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HA049S2Ar25"
   },
   "source": [
    "**Split Data into Train, Test & Validation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2dqKWAsNAq5L",
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.707876Z",
     "start_time": "2025-03-23T09:23:20.702487Z"
    }
   },
   "source": [
    "# Split into three parts: the training (67,4% of objects), the test (19,12% of objects),\n",
    "# and the validation set (13,48% of objects). In order to prevent overfitting of the model to the given data,\n",
    "# the test set contains selected frames from nine videos that were not used in either the training or validation sets.\n",
    "\n",
    "# Split image to : dataset/working/images\n",
    "# Split annotation to: dataset/working/labels\n",
    "\n",
    "def split_data(file_list, img_path, ann_path, mode):\n",
    "    #Check if we have our mode folders\n",
    "    images_working_folder = Path( 'dataset/working/images/'+  mode)\n",
    "    if not images_working_folder.exists():\n",
    "        print(f\"Path {images_working_folder} does not exit\")\n",
    "        os.makedirs(images_working_folder)\n",
    "\n",
    "    labels_working_folder = Path('dataset/working/labels/' + mode)\n",
    "    if not labels_working_folder.exists():\n",
    "        print(f\"Path {labels_working_folder} does not exit\")\n",
    "        os.makedirs(labels_working_folder)\n",
    "\n",
    "    #Creates the name of our label file from the img name and creates our source file\n",
    "    for file in file_list:\n",
    "        name = file.replace('.jpg', '')\n",
    "        img_src_file = str(img_path) + '/' + name + '.jpg'\n",
    "        annot_src_file = str(ann_path) + '/' + name + '.txt'\n",
    "        \n",
    "        if Path(img_src_file).exists() and Path(annot_src_file).exists():\n",
    "            #move image\n",
    "            IMG_DIR = 'dataset/working/images/' + mode\n",
    "            img_dest_file = str(IMG_DIR) + '/' + name + '.jpg'\n",
    "            if os.path.isfile(img_src_file) and not Path(img_dest_file).exists():\n",
    "                shutil.move(img_src_file, img_dest_file)\n",
    "    \n",
    "            # Copy annotations\n",
    "            ANNOT_DIR = 'dataset/working/labels/' + mode\n",
    "            annot_dest_file = str(ANNOT_DIR) + '/' + name + '.txt'\n",
    "            if os.path.isfile(annot_src_file) and not Path(annot_dest_file).exists():\n",
    "                shutil.move(annot_src_file, annot_dest_file)"
   ],
   "outputs": [],
   "execution_count": 302
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-g6Po3VCLX9",
    "outputId": "670cfbf0-8366-42d1-b630-b76363a3835a",
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.714355Z",
     "start_time": "2025-03-23T09:23:20.708674Z"
    }
   },
   "source": [
    "#Get our images list\n",
    "train_imgs = 'dataset/PART_1/PART_1/train.txt'\n",
    "test_imgs = 'dataset/PART_1/PART_1/test.txt'\n",
    "val_imgs = 'dataset/PART_1/PART_1/validation.txt'\n",
    "with open(train_imgs, 'r') as f:\n",
    "    train_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(test_imgs, 'r') as f:\n",
    "    test_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(val_imgs, 'r') as f:\n",
    "    val_img_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(train_img_list[0], test_img_list[0], val_img_list[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_102.jpg k2_38.jpg a_101.jpg\n"
     ]
    }
   ],
   "execution_count": 303
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IVYECDgBOsr",
    "outputId": "e73fc9e0-c326-42e0-fd29-e61c25f3b953",
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.814304Z",
     "start_time": "2025-03-23T09:23:20.747789Z"
    }
   },
   "source": [
    "# Root path\n",
    "root_img_path = Path('dataset/images/')\n",
    "root_ann_path = Path('dataset/annotations/')\n",
    "\n",
    "#Split Data\n",
    "split_data(train_img_list, root_img_path, root_ann_path, 'train')\n",
    "split_data(test_img_list, root_img_path, root_ann_path, 'test')\n",
    "split_data(val_img_list, root_img_path, root_ann_path, 'val')"
   ],
   "outputs": [],
   "execution_count": 304
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.837326Z",
     "start_time": "2025-03-23T09:23:20.815448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import os\n",
    "working_image_path = 'dataset/working/images/'\n",
    "working_labels_path = 'dataset/working/labels/'\n",
    "\n",
    "# Images\n",
    "img_test_path = glob.glob(os.path.join(working_image_path + '/test/' , \"*.jpg\"))\n",
    "print(f'img_test_path: {len(img_test_path)}')\n",
    "\n",
    "img_train_path = glob.glob(os.path.join(working_image_path + '/train/' , \"*.jpg\"))\n",
    "print(f'img_train_path: {len(img_train_path)}')\n",
    "\n",
    "img_val_path = glob.glob(os.path.join(working_image_path + '/val/' , \"*.jpg\"))\n",
    "print(f'img_val_path: {len(img_val_path)}')\n",
    "\n",
    "# Labels\n",
    "label_test_path = glob.glob(os.path.join(working_labels_path + '/test/' , \"*.txt\"))\n",
    "print(f'label_test_path: {len(label_test_path)}')\n",
    "\n",
    "label_train_path = glob.glob(os.path.join(working_labels_path + '/train/' , \"*.txt\"))\n",
    "print(f'label_train_path: {len(label_train_path)}')\n",
    "\n",
    "label_val_path = glob.glob(os.path.join(working_image_path + '/val/' , \"*.txt\"))\n",
    "print(f'label_val_path: {len(label_val_path)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_test_path: 514\n",
      "img_train_path: 2787\n",
      "img_val_path: 340\n",
      "label_test_path: 514\n",
      "label_train_path: 2787\n",
      "label_val_path: 0\n"
     ]
    }
   ],
   "execution_count": 305
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Custom Dataset class with handling for empty annotations\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_dir, classes, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.num_classes = len(classes)\n",
    "        self.image_files = [f for f in os.listdir(self.images_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        orig_width, orig_height = image.size\n",
    "        \n",
    "        # Store original size for later normalization\n",
    "        orig_size = (orig_height, orig_width)\n",
    "        \n",
    "        # Apply transformations to image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get image dimensions after transformation\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            img_height, img_width = image.shape[1:3]\n",
    "        else:\n",
    "            img_width, img_height = image.size\n",
    "            \n",
    "        # Load annotation\n",
    "        annotation_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "        \n",
    "        # Initialize empty tensors for bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # Parse annotation file if it exists\n",
    "        if os.path.exists(annotation_path):\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    data = line.strip().split()\n",
    "                    if len(data) == 5:\n",
    "                        class_id = int(data[0])\n",
    "                        \n",
    "                        # YOLO format: class_id, x_center, y_center, width, height (normalized)\n",
    "                        x_center = float(data[1])\n",
    "                        y_center = float(data[2])\n",
    "                        width = float(data[3])\n",
    "                        height = float(data[4])\n",
    "                        \n",
    "                        # Convert from YOLO format to pixel coordinates\n",
    "                        x_min = (x_center - width/2) * orig_width\n",
    "                        y_min = (y_center - height/2) * orig_height\n",
    "                        x_max = (x_center + width/2) * orig_width\n",
    "                        y_max = (y_center + height/2) * orig_height\n",
    "                        \n",
    "                        # Clip to image boundaries\n",
    "                        x_min = max(0, min(x_min, orig_width))\n",
    "                        y_min = max(0, min(y_min, orig_height))\n",
    "                        x_max = max(0, min(x_max, orig_width))\n",
    "                        y_max = max(0, min(y_max, orig_height))\n",
    "                        \n",
    "                        # Convert coordinates to the transformed image size\n",
    "                        x_min = x_min * (img_width / orig_width)\n",
    "                        y_min = y_min * (img_height / orig_height)\n",
    "                        x_max = x_max * (img_width / orig_width)\n",
    "                        y_max = y_max * (img_height / orig_height)\n",
    "                        \n",
    "                        boxes.append([x_min, y_min, x_max, y_max])\n",
    "                        labels.append(class_id)\n",
    "\n",
    "        # Handle case with no annotations\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'orig_size': torch.as_tensor(orig_size)\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# 2. Custom collate function to handle variable number of objects\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    for image, target in batch:\n",
    "        images.append(image)\n",
    "        targets.append(target)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "    return images, targets"
   ],
   "metadata": {
    "id": "k5rf17vWttIj",
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.845381Z",
     "start_time": "2025-03-23T09:23:20.838572Z"
    }
   },
   "outputs": [],
   "execution_count": 306
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.851857Z",
     "start_time": "2025-03-23T09:23:20.846398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Simple CNN-based object detection model with corrected dimensions\n",
    "from torchvision.models import resnet50\n",
    "\"\"\"\n",
    "    nn.Module: The base class for all neural network modules in PyTorch.\n",
    "\"\"\"\n",
    "class ImprovedDetector(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(ImprovedDetector, self).__init__()\n",
    "        \n",
    "        # Use ResNet50 as backbone with pretrained weights\n",
    "        self.backbone = resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the classification head\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        \n",
    "        # Feature pyramid network components\n",
    "        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1)\n",
    "        \n",
    "        # Lateral connections\n",
    "        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1)\n",
    "        self.latlayer2 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        \n",
    "        # Smooth layers\n",
    "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Detection head\n",
    "        self.detection_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.bbox_pred = nn.Conv2d(256, 4, kernel_size=3, padding=1)\n",
    "        self.cls_pred = nn.Conv2d(256, num_classes, kernel_size=3, padding=1)\n",
    "        self.objectness = nn.Conv2d(256, 1, kernel_size=3, padding=1)\n",
    "        \n",
    "    def _upsample_add(self, x, y):\n",
    "        \"\"\"Upsample and add two feature maps.\"\"\"\n",
    "        _, _, H, W = y.size()\n",
    "        return nn.functional.interpolate(x, size=(H, W), mode='bilinear', align_corners=False) + y\n",
    "        \n",
    "    def _make_fpn(self, c3, c4, c5):\n",
    "        \"\"\"Build FPN.\"\"\"\n",
    "        # Top-down\n",
    "        p5 = self.toplayer(c5)\n",
    "        p4 = self._upsample_add(p5, self.latlayer1(c4))\n",
    "        p3 = self._upsample_add(p4, self.latlayer2(c3))\n",
    "        \n",
    "        # Smooth\n",
    "        p4 = self.smooth1(p4)\n",
    "        p3 = self.smooth2(p3)\n",
    "        \n",
    "        return p3, p4, p5\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Get intermediate features\n",
    "        c3 = features[:, :512, :, :]\n",
    "        c4 = features[:, 512:1536, :, :]\n",
    "        c5 = features[:, 1536:, :, :]\n",
    "        \n",
    "        # Generate FPN features\n",
    "        p3, p4, p5 = self._make_fpn(c3, c4, c5)\n",
    "        \n",
    "        # Apply detection head to P3 (highest resolution feature map)\n",
    "        det_features = self.detection_head(p3)\n",
    "        \n",
    "        # Generate predictions\n",
    "        bbox_pred = self.bbox_pred(det_features)\n",
    "        cls_scores = self.cls_pred(det_features)\n",
    "        obj_score = torch.sigmoid(self.objectness(det_features))\n",
    "        \n",
    "        # Reshape outputs\n",
    "        batch_size = x.shape[0]\n",
    "        feature_h, feature_w = p3.shape[2], p3.shape[3]\n",
    "        \n",
    "        # Reshape bbox predictions\n",
    "        bbox_pred = bbox_pred.permute(0, 2, 3, 1).contiguous()\n",
    "        bbox_pred = bbox_pred.view(batch_size, -1, 4)\n",
    "        \n",
    "        # Reshape class predictions\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "        cls_scores = cls_scores.view(batch_size, -1, cls_scores.shape[1])\n",
    "        \n",
    "        # Reshape objectness predictions\n",
    "        obj_score = obj_score.permute(0, 2, 3, 1).contiguous()\n",
    "        obj_score = obj_score.view(batch_size, -1, 1)\n",
    "        \n",
    "        return {\n",
    "            'bbox': bbox_pred,\n",
    "            'cls': cls_scores,\n",
    "            'objectness': obj_score,\n",
    "            'feature_size': (feature_h, feature_w)\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 307
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.883557Z",
     "start_time": "2025-03-23T09:23:20.875844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.ops as ops\n",
    "# 4. Loss function for object detection\n",
    "class ImprovedDetectionLoss(nn.Module):\n",
    "    def __init__(self, lambda_coord=1.0, lambda_cls=1.0, lambda_obj=1.0, iou_threshold=0.5):\n",
    "        super(ImprovedDetectionLoss, self).__init__()\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_cls = lambda_cls\n",
    "        self.lambda_obj = lambda_obj\n",
    "        self.iou_threshold = iou_threshold\n",
    "        \n",
    "        # Loss functions\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def _generate_anchors(self, feature_size, input_size=(224, 224)):\n",
    "        \"\"\"Generate anchor boxes for each position in the feature map.\"\"\"\n",
    "        feature_h, feature_w = feature_size\n",
    "        input_h, input_w = input_size\n",
    "        \n",
    "        # Calculate stride\n",
    "        stride_h = input_h / feature_h\n",
    "        stride_w = input_w / feature_w\n",
    "        \n",
    "        # Generate grid centers\n",
    "        centers_h = torch.arange(0.5, feature_h, 1.0) * stride_h\n",
    "        centers_w = torch.arange(0.5, feature_w, 1.0) * stride_w\n",
    "        \n",
    "        # Create grid\n",
    "        centers_h, centers_w = torch.meshgrid(centers_h, centers_w)\n",
    "        centers = torch.stack((centers_w.flatten(), centers_h.flatten()), dim=1)\n",
    "        \n",
    "        # Define anchor sizes (can be tuned for specific dataset)\n",
    "        # Using a mix of small, medium, and large anchors\n",
    "        sizes = torch.tensor([[stride_h, stride_w], \n",
    "                              [stride_h*2, stride_w*2], \n",
    "                              [stride_h, stride_w*2], \n",
    "                              [stride_h*2, stride_w]])\n",
    "        \n",
    "        # Generate anchors for each center\n",
    "        anchors = []\n",
    "        for center in centers:\n",
    "            for size in sizes:\n",
    "                x_min = center[0] - size[0] / 2\n",
    "                y_min = center[1] - size[1] / 2\n",
    "                x_max = center[0] + size[0] / 2\n",
    "                y_max = center[1] + size[1] / 2\n",
    "                anchors.append([x_min, y_min, x_max, y_max])\n",
    "                \n",
    "        return torch.tensor(anchors, dtype=torch.float32)\n",
    "    \n",
    "    def _calculate_iou(self, boxes1, boxes2):\n",
    "        \"\"\"Calculate IoU between two sets of boxes.\"\"\"\n",
    "        # Calculate intersection\n",
    "        x_min = torch.max(boxes1[:, 0].unsqueeze(1), boxes2[:, 0].unsqueeze(0))\n",
    "        y_min = torch.max(boxes1[:, 1].unsqueeze(1), boxes2[:, 1].unsqueeze(0))\n",
    "        x_max = torch.min(boxes1[:, 2].unsqueeze(1), boxes2[:, 2].unsqueeze(0))\n",
    "        y_max = torch.min(boxes1[:, 3].unsqueeze(1), boxes2[:, 3].unsqueeze(0))\n",
    "        \n",
    "        intersection = torch.clamp(x_max - x_min, min=0) * torch.clamp(y_max - y_min, min=0)\n",
    "        \n",
    "        # Calculate union\n",
    "        boxes1_area = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "        boxes2_area = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "        \n",
    "        union = boxes1_area.unsqueeze(1) + boxes2_area.unsqueeze(0) - intersection\n",
    "        \n",
    "        return intersection / (union + 1e-6)\n",
    "    \n",
    "    def forward(self, predictions, targets, device='cpu'):\n",
    "        batch_size = predictions['bbox'].shape[0]\n",
    "        total_anchors = predictions['bbox'].shape[1]\n",
    "        feature_size = predictions['feature_size']\n",
    "        \n",
    "        # Initialize loss components\n",
    "        bbox_loss = torch.tensor(0.0, device=device)\n",
    "        cls_loss = torch.tensor(0.0, device=device)\n",
    "        obj_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Get predictions for this image\n",
    "            pred_bbox = predictions['bbox'][i]  # (anchors, 4)\n",
    "            pred_cls = predictions['cls'][i]    # (anchors, num_classes)\n",
    "            pred_obj = predictions['objectness'][i].squeeze(-1)  # (anchors)\n",
    "            \n",
    "            # Get target for this image\n",
    "            target = targets[i]\n",
    "            target_boxes = target['boxes'].to(device)  # (objects, 4)\n",
    "            target_labels = target['labels'].to(device)  # (objects)\n",
    "            \n",
    "            # Generate anchors\n",
    "            anchors = self._generate_anchors(feature_size).to(device)\n",
    "            \n",
    "            # If there are objects in this image\n",
    "            if len(target_boxes) > 0:\n",
    "                # Calculate IoU between anchors and ground truth boxes\n",
    "                ious = self._calculate_iou(anchors, target_boxes)  # (anchors, objects)\n",
    "                \n",
    "                # Assign anchors to objects\n",
    "                max_iou, best_target_idx = ious.max(dim=1)  # (anchors)\n",
    "                \n",
    "                # Positive anchors: IoU > threshold\n",
    "                positive_mask = max_iou > self.iou_threshold\n",
    "                \n",
    "                # Objectness loss\n",
    "                objectness_target = torch.zeros_like(pred_obj)\n",
    "                objectness_target[positive_mask] = 1.0\n",
    "                obj_loss += self.bce(pred_obj, objectness_target).mean()\n",
    "                \n",
    "                # For positive anchors only:\n",
    "                if positive_mask.sum() > 0:\n",
    "                    # Get target boxes and labels for positive anchors\n",
    "                    pos_best_target_idx = best_target_idx[positive_mask]\n",
    "                    pos_target_boxes = target_boxes[pos_best_target_idx]\n",
    "                    pos_target_labels = target_labels[pos_best_target_idx]\n",
    "                    \n",
    "                    # Box loss: GIoU loss for better convergence\n",
    "                    pos_pred_bbox = pred_bbox[positive_mask]\n",
    "                    bbox_loss += ops.generalized_box_iou_loss(\n",
    "                        pos_pred_bbox, \n",
    "                        pos_target_boxes,\n",
    "                        reduction='mean'\n",
    "                    )\n",
    "                    \n",
    "                    # Classification loss\n",
    "                    pos_pred_cls = pred_cls[positive_mask]\n",
    "                    cls_loss += self.ce(pos_pred_cls, pos_target_labels).mean()\n",
    "            else:\n",
    "                # No objects - objectness should be 0\n",
    "                obj_loss += self.bce(pred_obj, torch.zeros_like(pred_obj)).mean()\n",
    "                \n",
    "        # Compute total loss\n",
    "        total_loss = (\n",
    "            self.lambda_coord * bbox_loss + \n",
    "            self.lambda_cls * cls_loss + \n",
    "            self.lambda_obj * obj_loss\n",
    "        ) / batch_size\n",
    "        \n",
    "        # Return individual loss components for monitoring\n",
    "        return total_loss, {\n",
    "            'bbox_loss': bbox_loss / batch_size,\n",
    "            'cls_loss': cls_loss / batch_size,\n",
    "            'obj_loss': obj_loss / batch_size\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": 308
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:20.929964Z",
     "start_time": "2025-03-23T09:23:20.923540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Training function\n",
    "def train_model(model, train_loader, val_loader=None, criterion=None, optimizer=None, \n",
    "                scheduler=None, num_epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Initialize loss history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'bbox_loss': [],\n",
    "        'cls_loss': [],\n",
    "        'obj_loss': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 20)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_bbox_loss = 0.0\n",
    "        running_cls_loss = 0.0\n",
    "        running_obj_loss = 0.0\n",
    "        \n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                     for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, loss_components = criterion(predictions, targets, device)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item()\n",
    "            running_bbox_loss += loss_components['bbox_loss'].item()\n",
    "            running_cls_loss += loss_components['cls_loss'].item()\n",
    "            running_obj_loss += loss_components['obj_loss'].item()\n",
    "            \n",
    "            # Print stats every 10 batches\n",
    "            if (i+1) % 10 == 0:\n",
    "                batch_loss = running_loss / 10\n",
    "                batch_bbox_loss = running_bbox_loss / 10\n",
    "                batch_cls_loss = running_cls_loss / 10\n",
    "                batch_obj_loss = running_obj_loss / 10\n",
    "                \n",
    "                print(f'  Batch {i+1}/{len(train_loader)} | '\n",
    "                      f'Loss: {batch_loss:.4f} | '\n",
    "                      f'Box: {batch_bbox_loss:.4f} | '\n",
    "                      f'Cls: {batch_cls_loss:.4f} | '\n",
    "                      f'Obj: {batch_obj_loss:.4f}')\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_bbox_loss = 0.0\n",
    "                running_cls_loss = 0.0\n",
    "                running_obj_loss = 0.0\n",
    "        \n",
    "        # Validation phase\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, targets in val_loader:\n",
    "                    images = images.to(device)\n",
    "                    targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                             for k, v in t.items()} for t in targets]\n",
    "                    \n",
    "                    predictions = model(images)\n",
    "                    loss, _ = criterion(predictions, targets, device)\n",
    "                    \n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    val_samples += images.size(0)\n",
    "            \n",
    "            val_loss = val_loss / val_samples\n",
    "            history['val_loss'].append(val_loss)\n",
    "            \n",
    "            print(f'  Validation Loss: {val_loss:.4f}')\n",
    "            \n",
    "            # Update learning rate based on validation loss\n",
    "            if scheduler:\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "            # Save best model\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print('  New best model saved!')\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n",
    "    print('Training complete')\n",
    "    \n",
    "    return model, history"
   ],
   "outputs": [],
   "execution_count": 309
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Train model**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create Dataset and DatasetLoader"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T09:23:21.018117Z",
     "start_time": "2025-03-23T09:23:20.945471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "    batch_size (int): Number of images processed in one forward/backward pass.\n",
    "    num_epochs (int): Number of times the model will iterate over the entire dataset.\n",
    "    learning_rate (float): Step size for updating model weights during optimization.\n",
    "\"\"\"\n",
    "train_images_dir = 'dataset/working/images/train/'\n",
    "train_label_dir = 'dataset/working/labels/train/'\n",
    "val_images_dir = 'dataset/working/images/val/'  # Add validation set if available\n",
    "val_label_dir = 'dataset/working/labels/val/'\n",
    "classes = ['human', 'wind/sup-board', 'boat', 'bouy', 'sailboat', 'kayak']\n",
    "batch_size = 8\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001  # Reduced learning rate\n",
    "input_size = (224, 224) \n",
    "# Set device\n",
    "\"\"\"\n",
    "Checks if a GPU (cuda) is available and sets the device accordingly. \n",
    "If not, it defaults to the CPU.\n",
    "Reason: GPUs are much faster for deep learning tasks due to their parallel processing capabilities.\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = ObjectDetectionDataset(\n",
    "    images_dir=train_images_dir,\n",
    "    annotations_dir=train_label_dir,\n",
    "    classes=classes,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Print dataset size\n",
    "print(f\"Dataset size: {len(train_dataset)} images\")\n",
    "\n",
    "# Create validation dataset if directories exist\n",
    "val_dataset = None\n",
    "if os.path.exists(val_images_dir) and os.path.exists(val_label_dir):\n",
    "    val_dataset = ObjectDetectionDataset(\n",
    "        images_dir=val_images_dir,\n",
    "        annotations_dir=val_label_dir,\n",
    "        classes=classes,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    " # Print dataset size\n",
    "print(f\"Training dataset size: {len(train_dataset)} images\")\n",
    "if val_dataset:\n",
    "    print(f\"Validation dataset size: {len(val_dataset)} images\")\n",
    "\n",
    "# Use custom collate_fn to handle variable number of objects\n",
    "\"\"\"\n",
    "Args:\n",
    "    batch_size (int): Number of images processed in one forward/backward pass.\n",
    "    shuffle: Shuffles the dataset at the start of each epoch to ensure randomness.\n",
    "    num_workers (int): Number of subprocesses for data loading. Set to 0 for single-threaded loading.\n",
    "    collate_fn: A custom function to handle variable numbers of objects in different images.\n",
    "\"\"\"\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4 if device.type == 'cuda' else 0,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "    \n",
    "val_loader = None\n",
    "if val_dataset:\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4 if device.type == 'cuda' else 0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "\n",
    "# Check first sample to verify data loading\n",
    "sample_img, sample_target = train_dataset[0]\n",
    "print(f\"Sample image shape: {sample_img.shape}\")\n",
    "print(f\"Sample target boxes shape: {sample_target['boxes'].shape}\")\n",
    "print(f'Sample target boxes value: {sample_target['boxes']}')\n",
    "print(f\"Sample target labels: {sample_target['labels'].shape}\")\n",
    "print(f\"Sample target labels values: {sample_target['labels']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset size: 2787 images\n",
      "Training dataset size: 2787 images\n",
      "Validation dataset size: 340 images\n",
      "Sample image shape: torch.Size([3, 224, 224])\n",
      "Sample target boxes shape: torch.Size([34, 4])\n",
      "Sample target boxes value: tensor([[4.0279e+01, 6.8652e+01, 4.4771e+01, 7.6948e+01],\n",
      "        [1.4788e+02, 1.8636e+02, 1.5173e+02, 1.9403e+02],\n",
      "        [1.3977e+02, 1.6655e+02, 1.4502e+02, 1.7982e+02],\n",
      "        [4.3721e+01, 1.2994e+02, 4.9612e+01, 1.4176e+02],\n",
      "        [7.5804e+01, 1.3279e+02, 8.0179e+01, 1.4078e+02],\n",
      "        [1.1792e+02, 1.0718e+02, 1.2346e+02, 1.1952e+02],\n",
      "        [1.3420e+02, 1.6141e+02, 1.3904e+02, 1.7189e+02],\n",
      "        [1.2542e+02, 1.4213e+02, 1.2903e+02, 1.5156e+02],\n",
      "        [2.1204e+01, 4.7185e+00, 2.7212e+01, 1.3741e+01],\n",
      "        [1.4467e+01, 1.5556e-01, 1.9483e+01, 4.6148e+00],\n",
      "        [1.0684e+02, 1.0059e+02, 1.1250e+02, 1.1221e+02],\n",
      "        [4.2204e+01, 8.7993e+01, 4.7046e+01, 9.8674e+01],\n",
      "        [7.5571e+01, 1.6733e+02, 7.9363e+01, 1.7282e+02],\n",
      "        [7.8604e+01, 1.6665e+02, 8.2396e+01, 1.7246e+02],\n",
      "        [1.4668e+02, 1.7381e+02, 1.5864e+02, 1.9932e+02],\n",
      "        [1.0558e+02, 1.3611e+02, 1.1037e+02, 1.4327e+02],\n",
      "        [7.3617e+01, 8.2963e+01, 7.9450e+01, 9.6859e+01],\n",
      "        [6.0608e+01, 7.8659e+01, 7.0058e+01, 1.0573e+02],\n",
      "        [9.3275e+01, 1.0438e+02, 9.9225e+01, 1.1547e+02],\n",
      "        [3.5875e+01, 6.7044e+01, 4.5208e+01, 9.4526e+01],\n",
      "        [3.5904e+01, 8.1874e+01, 4.0162e+01, 8.7785e+01],\n",
      "        [3.8617e+01, 7.5341e+01, 4.3167e+01, 8.3119e+01],\n",
      "        [3.5729e+01, 1.4197e+02, 4.1737e+01, 1.4944e+02],\n",
      "        [5.1508e+01, 1.4171e+02, 6.4808e+01, 1.5384e+02],\n",
      "        [5.3521e+01, 1.4856e+02, 5.8713e+01, 1.5613e+02],\n",
      "        [4.0279e+01, 1.4379e+02, 5.3287e+01, 1.6193e+02],\n",
      "        [3.5292e+01, 1.5794e+02, 4.6725e+01, 1.7287e+02],\n",
      "        [6.6150e+01, 9.7741e+01, 7.1167e+01, 1.0739e+02],\n",
      "        [6.2067e+01, 8.5607e+01, 6.6617e+01, 9.3178e+01],\n",
      "        [5.9879e+01, 7.9126e+01, 6.5188e+01, 8.5763e+01],\n",
      "        [6.0200e+01, 9.7170e+01, 6.5567e+01, 1.0671e+02],\n",
      "        [5.7692e+01, 9.1985e+01, 6.3875e+01, 1.0070e+02],\n",
      "        [1.1288e+02, 7.9644e+01, 1.1731e+02, 8.7319e+01],\n",
      "        [1.1740e+02, 8.3378e+01, 1.2177e+02, 9.3748e+01]])\n",
      "Sample target labels: torch.Size([34])\n",
      "Sample target labels values: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "execution_count": 310
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Configuration\n",
    "# 6. Main function to set up and run the training\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Create model, loss function, and optimizer\n",
    "    # Create model, loss function, and optimizer\n",
    "    model = ImprovedDetector(num_classes=len(classes))\n",
    "    criterion = ImprovedDetectionLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "     \n",
    "    # Print model summary\n",
    "    print(\"\\nModel Structure:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Try a forward pass with a batch to check dimensions\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    if val_loader:\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['bbox_loss'], label='Box Loss')\n",
    "    plt.plot(history['cls_loss'], label='Class Loss')\n",
    "    plt.plot(history['obj_loss'], label='Object Loss')\n",
    "    plt.title('Loss Components')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRGsOIrLty4z",
    "outputId": "7fd32aad-d4f5-4daa-f87c-4f3cac1310e5",
    "ExecuteTime": {
     "end_time": "2025-03-23T09:24:18.355844Z",
     "start_time": "2025-03-23T09:23:21.019385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/truongngo/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/truongngo/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/truongngo/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100.0%\n",
      "/Users/truongngo/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Structure:\n",
      "ImprovedDetector(\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (toplayer): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (latlayer1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (latlayer2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (smooth1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (smooth2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (detection_head): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "  )\n",
      "  (bbox_pred): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (cls_pred): Conv2d(256, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (objectness): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "Epoch 1/20\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 2048, 1, 1], expected input[8, 512, 7, 7] to have 2048 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[311]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28mprint\u001B[39m(model)\n\u001B[32m     18\u001B[39m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m trained_model, history = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# Try a forward pass with a batch to check dimensions\u001B[39;00m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# Plot training history\u001B[39;00m\n\u001B[32m     32\u001B[39m plt.figure(figsize=(\u001B[32m12\u001B[39m, \u001B[32m4\u001B[39m))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[309]\u001B[39m\u001B[32m, line 37\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\u001B[39m\n\u001B[32m     34\u001B[39m optimizer.zero_grad()\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m predictions = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[38;5;66;03m# Calculate loss\u001B[39;00m\n\u001B[32m     40\u001B[39m loss, loss_components = criterion(predictions, targets, device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[307]\u001B[39m\u001B[32m, line 72\u001B[39m, in \u001B[36mImprovedDetector.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     69\u001B[39m c5 = features[:, \u001B[32m1536\u001B[39m:, :, :]\n\u001B[32m     71\u001B[39m \u001B[38;5;66;03m# Generate FPN features\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m p3, p4, p5 = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_fpn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc4\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc5\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[38;5;66;03m# Apply detection head to P3 (highest resolution feature map)\u001B[39;00m\n\u001B[32m     75\u001B[39m det_features = \u001B[38;5;28mself\u001B[39m.detection_head(p3)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[307]\u001B[39m\u001B[32m, line 52\u001B[39m, in \u001B[36mImprovedDetector._make_fpn\u001B[39m\u001B[34m(self, c3, c4, c5)\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Build FPN.\"\"\"\u001B[39;00m\n\u001B[32m     51\u001B[39m \u001B[38;5;66;03m# Top-down\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m p5 = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtoplayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc5\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m p4 = \u001B[38;5;28mself\u001B[39m._upsample_add(p5, \u001B[38;5;28mself\u001B[39m.latlayer1(c4))\n\u001B[32m     54\u001B[39m p3 = \u001B[38;5;28mself\u001B[39m._upsample_add(p4, \u001B[38;5;28mself\u001B[39m.latlayer2(c3))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/CloudStorage/OneDrive-Personal/Document/USTH Bachelor/USTH-B3/Machine Leaning 2/Labworks/Final/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    537\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.padding_mode != \u001B[33m\"\u001B[39m\u001B[33mzeros\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Given groups=1, weight of size [256, 2048, 1, 1], expected input[8, 512, 7, 7] to have 2048 channels, but got 512 channels instead"
     ]
    }
   ],
   "execution_count": 311
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Validate model**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_model(model_path, num_classes, input_size):\n",
    "    \"\"\"\n",
    "    Load a trained model from a saved state dict\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model\n",
    "        num_classes (int): Number of object classes\n",
    "        input_size (tuple): Input image size (height, width)\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded model\n",
    "    \"\"\"\n",
    "    model = SimpleDetector(num_classes=num_classes, input_size=input_size)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_image(image_path, input_size):\n",
    "    \"\"\"\n",
    "    Preprocess an image for inference\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the input image\n",
    "        input_size (tuple): Input size for the model (height, width)\n",
    "    \n",
    "    Returns:\n",
    "        tensor_image: Preprocessed image tensor\n",
    "        original_image: Original PIL image\n",
    "        scale_factor: Scale factor between original and resized image\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    original_size = original_image.size  # (width, height)\n",
    "    \n",
    "    # Calculate scale factors for converting coordinates back to original image\n",
    "    scale_x = original_size[0] / input_size[1]\n",
    "    scale_y = original_size[1] / input_size[0]\n",
    "    scale_factor = (scale_x, scale_y)\n",
    "    \n",
    "    # Apply the same transformations used during training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    tensor_image = transform(original_image).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    return tensor_image, original_image, scale_factor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "def detect_objects_model(model, image_tensor, confidence_threshold=0.5, nms_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Perform object detection on an image\n",
    "    \n",
    "    Args:\n",
    "        model: The trained object detection model\n",
    "        image_tensor: Preprocessed image tensor\n",
    "        confidence_threshold: Minimum confidence for valid detections\n",
    "        nms_threshold: Non-maximum suppression threshold\n",
    "    \n",
    "    Returns:\n",
    "        boxes: Detected bounding boxes\n",
    "        labels: Detected class labels\n",
    "        scores: Confidence scores\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "    \n",
    "    # Extract predictions\n",
    "    pred_bbox = outputs['bbox'][0]  # Remove batch dimension\n",
    "    pred_cls = outputs['cls'][0]\n",
    "    pred_objectness = outputs['objectness'][0]\n",
    "    \n",
    "    # Print shape information for debugging\n",
    "    print(f\"Prediction shapes - bbox: {pred_bbox.shape}, cls: {pred_cls.shape}, objectness: {pred_objectness.shape}\")\n",
    "    \n",
    "    # Check if pred_bbox has the correct shape (should have 4 values per box)\n",
    "    if pred_bbox.dim() == 1:\n",
    "        # If it's a flat tensor, reshape it based on known number of bbox coordinates (4)\n",
    "        pred_bbox = pred_bbox.view(-1, 4)\n",
    "        print(f\"Reshaped flat bbox to: {pred_bbox.shape}\")\n",
    "    elif pred_bbox.dim() > 2:\n",
    "        # If pred_bbox has more than 2 dimensions, reshape it\n",
    "        pred_bbox = pred_bbox.reshape(-1, pred_bbox.shape[-1])\n",
    "        print(f\"Reshaped bbox to: {pred_bbox.shape}\")\n",
    "    \n",
    "    # Make sure each box has 4 coordinates\n",
    "    if pred_bbox.shape[1] != 4:\n",
    "        print(f\"Warning: bbox does not have 4 coordinates per box. Current shape: {pred_bbox.shape}\")\n",
    "        # If we can't fix this, we'll filter out invalid boxes later\n",
    "    \n",
    "    if pred_cls.dim() > 2:\n",
    "        # If pred_cls has more than 2 dimensions, reshape it\n",
    "        pred_cls = pred_cls.reshape(-1, pred_cls.shape[-1])\n",
    "        print(f\"Reshaped cls to: {pred_cls.shape}\")\n",
    "    \n",
    "    if pred_objectness.dim() > 1:\n",
    "        # If pred_objectness has more than 1 dimension, flatten it\n",
    "        pred_objectness = pred_objectness.reshape(-1)\n",
    "        print(f\"Reshaped objectness to: {pred_objectness.shape}\")\n",
    "    \n",
    "    # Make sure all tensors have consistent first dimension\n",
    "    min_length = min(pred_bbox.shape[0], pred_cls.shape[0] if pred_cls.dim() > 1 else len(pred_cls), \n",
    "                    len(pred_objectness))\n",
    "    \n",
    "    print(f\"Using min_length: {min_length}\")\n",
    "    \n",
    "    if pred_bbox.shape[0] > min_length:\n",
    "        pred_bbox = pred_bbox[:min_length]\n",
    "    \n",
    "    if pred_cls.dim() > 1 and pred_cls.shape[0] > min_length:\n",
    "        pred_cls = pred_cls[:min_length]\n",
    "    elif pred_cls.dim() == 1 and len(pred_cls) > min_length:\n",
    "        pred_cls = pred_cls[:min_length]\n",
    "    \n",
    "    if len(pred_objectness) > min_length:\n",
    "        pred_objectness = pred_objectness[:min_length]\n",
    "    \n",
    "    # Get class scores\n",
    "    if pred_cls.dim() > 1:\n",
    "        # Multi-class case\n",
    "        class_scores, class_indices = torch.max(pred_cls, dim=1)\n",
    "    else:\n",
    "        # Single-class case\n",
    "        class_scores = pred_cls\n",
    "        class_indices = torch.zeros_like(class_scores, dtype=torch.long)\n",
    "    \n",
    "    # Calculate confidence scores\n",
    "    confidence_scores = pred_objectness * class_scores\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    mask = confidence_scores > confidence_threshold\n",
    "    \n",
    "    # Ensure mask length matches the tensors\n",
    "    if len(mask) != pred_bbox.shape[0]:\n",
    "        print(f\"Warning: Mask length {len(mask)} doesn't match bbox length {pred_bbox.shape[0]}\")\n",
    "        # Adjust mask to match the tensor size\n",
    "        if len(mask) > pred_bbox.shape[0]:\n",
    "            mask = mask[:pred_bbox.shape[0]]\n",
    "        else:\n",
    "            # If mask is too short, extend it with False values\n",
    "            padding = torch.zeros(pred_bbox.shape[0] - len(mask), dtype=torch.bool, device=mask.device)\n",
    "            mask = torch.cat([mask, padding])\n",
    "    \n",
    "    # Apply mask\n",
    "    boxes = pred_bbox[mask]\n",
    "    labels = class_indices[mask] if len(class_indices) == len(mask) else torch.zeros(sum(mask), dtype=torch.long)\n",
    "    scores = confidence_scores[mask]\n",
    "    \n",
    "    print(f\"After filtering: {len(boxes)} boxes remaining\")\n",
    "    \n",
    "    # Validate that all boxes have 4 coordinates\n",
    "    valid_boxes_mask = torch.ones(boxes.shape[0], dtype=torch.bool)\n",
    "    \n",
    "    if boxes.dim() == 1:  # If boxes somehow ended up as a 1D tensor\n",
    "        print(\"Warning: boxes is a 1D tensor. Reshaping...\")\n",
    "        if boxes.shape[0] % 4 == 0:  # If divisible by 4, reshape\n",
    "            boxes = boxes.reshape(-1, 4)\n",
    "        else:\n",
    "            # Can't reshape cleanly, we'll need to filter out bad boxes\n",
    "            valid_boxes = []\n",
    "            valid_labels = []\n",
    "            valid_scores = []\n",
    "            \n",
    "            for i in range(len(boxes)):\n",
    "                if isinstance(boxes[i], torch.Tensor) and boxes[i].numel() == 4:\n",
    "                    valid_boxes.append(boxes[i])\n",
    "                    valid_labels.append(labels[i])\n",
    "                    valid_scores.append(scores[i])\n",
    "            \n",
    "            # If we found valid boxes, stack them\n",
    "            if valid_boxes:\n",
    "                boxes = torch.stack(valid_boxes)\n",
    "                labels = torch.stack(valid_labels)\n",
    "                scores = torch.stack(valid_scores)\n",
    "            else:\n",
    "                # No valid boxes, return empty tensors\n",
    "                return torch.zeros((0, 4)), torch.zeros(0, dtype=torch.long), torch.zeros(0)\n",
    "    \n",
    "    # Check if boxes has the correct shape after filtering\n",
    "    if boxes.shape[1] != 4:\n",
    "        print(f\"Warning: Filtered boxes don't have 4 coordinates. Shape: {boxes.shape}\")\n",
    "        # Create valid boxes mask\n",
    "        for i in range(boxes.shape[0]):\n",
    "            if boxes[i].numel() != 4:\n",
    "                valid_boxes_mask[i] = False\n",
    "        \n",
    "        # Apply valid boxes mask\n",
    "        if not torch.all(valid_boxes_mask):\n",
    "            boxes = boxes[valid_boxes_mask]\n",
    "            labels = labels[valid_boxes_mask]\n",
    "            scores = scores[valid_boxes_mask]\n",
    "    \n",
    "    # Apply non-maximum suppression if we have more than one box\n",
    "    if len(boxes) > 1:\n",
    "        # Ensure boxes are in correct format for NMS (some models output x,y,w,h instead of x1,y1,x2,y2)\n",
    "        if boxes.shape[1] == 4:\n",
    "            # Check if format is x,y,w,h by seeing if x2,y2 values are always larger than x1,y1\n",
    "            x1y1 = boxes[:, :2]\n",
    "            x2y2 = boxes[:, 2:]\n",
    "            if not torch.all(x2y2 > x1y1):\n",
    "                # Convert from x,y,w,h to x1,y1,x2,y2\n",
    "                x1y1 = boxes[:, :2]\n",
    "                wh = boxes[:, 2:]\n",
    "                x2y2 = x1y1 + wh\n",
    "                boxes = torch.cat([x1y1, x2y2], dim=1)\n",
    "                \n",
    "        try:\n",
    "            # Apply NMS\n",
    "            indices = torchvision.ops.nms(boxes, scores, nms_threshold)\n",
    "            boxes = boxes[indices]\n",
    "            labels = labels[indices]\n",
    "            scores = scores[indices]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during NMS: {e}\")\n",
    "            # If NMS fails, return all detections\n",
    "            pass\n",
    "    \n",
    "    return boxes, labels, scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "def draw_detections(image, boxes, labels, scores, classes, scale_factor=(1.0, 1.0)):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on the image\n",
    "    \n",
    "    Args:\n",
    "        image: PIL image to draw on\n",
    "        boxes: Detected bounding boxes\n",
    "        labels: Detected class labels\n",
    "        scores: Confidence scores\n",
    "        classes: List of class names\n",
    "        scale_factor: Scale factor to convert coordinates from model input to original image\n",
    "    \n",
    "    Returns:\n",
    "        annotated_image: Image with annotations\n",
    "    \"\"\"\n",
    "    # Create a copy of the image to draw on\n",
    "    draw_image = image.copy()\n",
    "    draw = ImageDraw.Draw(draw_image)\n",
    "    \n",
    "    # Try to load a font, use default if not available\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Generate distinct colors for each class\n",
    "    colors = {}\n",
    "    for i in range(len(classes)):\n",
    "        hue = i / max(1, len(classes) - 1)\n",
    "        rgb = hsv_to_rgb((hue, 0.7, 0.7))  # Convert HSV to RGB\n",
    "        colors[i] = (int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255))\n",
    "    \n",
    "    # Draw each detection\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i].tolist()\n",
    "        \n",
    "        # Scale box coordinates back to original image size\n",
    "        x1, y1, x2, y2 = box\n",
    "        print(\"Bounding Box:\", x1, y1, x2, y2)\n",
    "        # x1 *= scale_factor[0]\n",
    "        # y1 *= scale_factor[1]\n",
    "        # x2 *= scale_factor[0]\n",
    "        # y2 *= scale_factor[1]\n",
    "        \n",
    "        # Ensure box coordinates are integers and within image boundaries\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        \n",
    "        # Get label and score\n",
    "        label_idx = int(labels[i].item()) if labels.numel() > 0 else 0\n",
    "        score = scores[i].item()\n",
    "        \n",
    "        # Ensure label_idx is in range\n",
    "        label_idx = min(label_idx, len(classes) - 1)\n",
    "        \n",
    "        # Draw bounding box\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=colors[label_idx], width=3)\n",
    "        \n",
    "        # Prepare and draw label\n",
    "        class_name = classes[label_idx]\n",
    "        label_text = f\"{class_name}: {score:.2f}\"\n",
    "        print(f'Label: {label_text}')\n",
    "        \n",
    "        # Get text size\n",
    "        try:\n",
    "            text_size = draw.textbbox((0, 0), label_text, font=font)[2:4]\n",
    "        except AttributeError:\n",
    "            # For older PIL versions\n",
    "            text_size = draw.textsize(label_text, font=font)\n",
    "        \n",
    "        # Draw label background\n",
    "        draw.rectangle(\n",
    "            [(x1, max(0, y1 - text_size[1] - 4)), (x1 + text_size[0] + 4, y1)],\n",
    "            fill=colors[label_idx]\n",
    "        )\n",
    "        \n",
    "        # Draw label text\n",
    "        draw.text((x1 + 2, max(0, y1 - text_size[1] - 2)), label_text, fill=\"white\", font=font)\n",
    "    \n",
    "    return draw_image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_detections(image_path, model_path, classes, input_size, conf_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Main function to load a model and visualize detections\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the input image\n",
    "        model_path (str): Path to the saved model\n",
    "        classes (list): List of class names\n",
    "        input_size (tuple): Model input size (height, width)\n",
    "        conf_threshold (float): Confidence threshold for detections\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = load_model(model_path, num_classes=len(classes), input_size=input_size)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Processing image: {image_path}\")\n",
    "    \n",
    "    # Preprocess the image\n",
    "    image_tensor, original_image, scale_factor = preprocess_image(image_path, input_size)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    print(f\"Input image shape: {image_tensor}\")\n",
    "    print(f\"Input image size: {original_image}\")\n",
    "    print(f\"Scale image shape: {scale_factor}\")\n",
    "    \n",
    "    # Detect objects\n",
    "    try:\n",
    "        boxes, labels, scores = detect_objects_model(model, image_tensor, conf_threshold)\n",
    "        print(f\"boxes: {boxes}\")\n",
    "        print(f\"Detected {len(boxes)} objects\")\n",
    "        \n",
    "        # Draw detections on image\n",
    "        annotated_image = draw_detections(original_image, boxes, labels, scores, classes, scale_factor)\n",
    "         \n",
    "        # Save the result\n",
    "        output_path = image_path.replace('.', '_detected.')\n",
    "        annotated_image.save(output_path)\n",
    "        print(f\"Annotated image saved to {output_path}\")\n",
    "\n",
    "        # Display the result\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(np.array(annotated_image))\n",
    "        plt.axis('off')\n",
    "        plt.title('Object Detection Results')\n",
    "        plt.show()\n",
    "\n",
    "        return annotated_image\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during detection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace with your actual values\n",
    "model_path = 'object_detection_model.pth'\n",
    "classes = ['human', 'wind/sup-board', 'boat', 'bouy', 'sailboat', 'kayak']\n",
    "input_size = (224, 224)  # Height, Width\n",
    "\n",
    "# Example: Process a single image\n",
    "image_path = 'dataset/working/images/val/a_101.jpg'\n",
    "result = visualize_detections(image_path, model_path, classes, input_size)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
